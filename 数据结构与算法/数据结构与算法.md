# 极客时间-数据结构与算法

** 内存泄漏是指程序在申请内存后，无法释放已申请的内存空间。内存溢出是指程序申请内存时，没有足够的内存供申请者使用；或者说提供一块存储int数据的存储空间，但存储了long数据，则结果是内存不够用，报错OOM。内存泄漏的堆积最终会导致内存溢出。**

内存溢出（Out Of Memory） ：就是申请内存时，JVM没有足够的内存空间。通俗说法就是去蹲坑发现坑位满了。
内存泄露 （Memory Leak）：就是申请了内存，但是没有释放，导致内存空间浪费。通俗说法就是有人占着茅坑不拉屎。

## # 入门篇

#### 时间复杂度
所有代码的执行时间 T(n) 与每行代码的执行次数 f(n) 成正比。
T(n) = O(f(n))
其中，T(n) 它表示代码执行的时间；n 表示数据规模的大小；f(n) 表示每行代码执行的次数总和。因为这是一个公式，所以用 f(n) 来表示。公式中的 O，表示代码的执行时间 T(n) 与 f(n) 表达式成正比。

* 定义：** 大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。**
* 常见复杂度： O(1)、O(logn)、O(n)、O(nlogn)、O(n^2 )
* 时间复杂度类别：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度
* 对数相关： 若：2^0  2^1  2^2  ...  2^k  ...  2^x = n, 则: x=log2n(读作log以2为底n的对数);
* 对数运算: log3n = log32 * log2n=

#### 空间复杂度

* 空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系
* 常见复杂度: O(1)、O(n)、O(n^2 )


## # 数组

#### 必会算法

#### 章节内容
数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。

* -注意点：
    1. 切忌将数组的查找时间复杂度认为是O(1)，无序数组的查找时间复杂度应该是O(n)，有序的二分查找的时间复杂度是O(logn),，正确的描述应该是数组随机访问下标对应元素的时间复杂度是O(1)


* -使用技巧：
假设数组arr有100W个元素，数组长度是110W，现在需要删除arr[5]、arr[6]、arr[7]，如果接到删除命令就开始删除数据，数组为了保证地址连续性，没删除一个数组元素则会移动数组进百万次，如果当前系统处于繁忙时段，再执行这个删除逻辑的话将会使计算机资源使用更紧张甚至进程崩溃宕机；如果删除时，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。

* -数组为啥下标从0开始，而不是从1开始？（可能的原因之一）
从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。如果用 a 来表示数组的首地址，a[0]就是偏移为 0 的位置，也就是首地址，a[k]就表示偏移 k 个 type_size 的位置，所以计算 a[k]的内存地址只需要用这个公式：a[k]_address = base_address + k * type_size
但是，如果数组从 1 开始计数，那我们计算数组元素 a[k]的内存地址就会变为：a[k]_address = base_address + (k-1)*type_size对比两个公式，我们不难发现，从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。


## # 链表

#### 必会算法
1. 单链表反转
2. 链表中环的检测
3. 两个有序的链表合并
4. 删除链表倒数第 n 个结点
5. 求链表的中间结点

#### 章节内容
空间换时间与时间换空间：对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗。


## # 递归
递归会遇到的问题：1. 栈溢出；2. 重复计算(可以考虑用hash表等来存储计算过的值)

#### 必会算法
1. 每次只能爬1个台阶或者2个台阶，问爬n个台阶有多少种方法？ （使用hash表记录计算过的值, 如：计算f(5) = f(4) + f(3), 计算f(6) = f(5) + f(4), 所以记录f(5)、f(4)、f(3), 之后就不用再重复计算了）

#### 章节内容

写递归代码最关键的是写出递推公式，找到终止条件，剩下将递推公式转化为代码就很简单了。

1. 一个问题的解可以分解为几个子问题的解
2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样
3. 存在递归终止条件

总结一下，写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。

#### 问题
那是不是所有的递归代码都可以改为这种迭代循环的非递归写法呢？
笼统地讲，是的。因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。但是这种思路实际上是将递归改为了“手动”递归，本质并没有变，而且也并没有解决前面讲到的某些问题，徒增了实现的复杂度。


## # 排序上

#### 章节内容
对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：

1. 最好情况、最坏情况、平均情况时间复杂度
我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。为什么要区分这三种时间复杂度呢？第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。
2. 时间复杂度的系数、常数 、低阶
我们知道，时间复杂度反映的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是 10 个、100 个、1000 个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。
3. 比较次数和交换（或移动）次数
这一节和下一节讲的都是基于比较的排序算法。基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。

#### 算法
1. 冒泡排序 ( O(n^2) )

```golang

// 冒泡排序
func bubbleSort(arr []int32) (res []int32) {
	var l = len(arr)
	if l <= 1 {
		res = arr
		return
	}

	for i := 0; i < l; i++ {
		var flag = false // 外层循环未结束，但是已经有序
		for j := 0; j < l-i-1; j++ {
			if arr[j] > arr[j+1] {
				arr[j], arr[j+1] = arr[j+1], arr[j]
				flag = true
			}
		}
		if !flag {
			break
		}
	}
	res = arr
	return
}

```

2. 插入排序 ( O(n^2) )

```golang



```

3. 选择排序 ( O(n^2) )

```golang



```

4. 快速排序 ( O(nlogn) )

```golang



```

5. 归并排序 ( O(nlogn) )

```golang



```

7. 桶排序   ( O(n) )

```golang



```

8. 计算排序 ( O(n) )

```golang



```

9. 基数排序 ( O(n) )

```golang



```

## # 排序下