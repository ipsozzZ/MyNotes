# 来自极客课程《趣谈linux操作系统》笔记
开机、关机： shutdown -h now是现在就关机，reboot就是重启。

## # 开篇

#### 用公司发展的角度理解操作系统

1. 初创期：这个老板基于开放的营商环境（x86 体系结构），创办一家外包公司（系统的启动）。因为一开始没有其他员工，老板需要亲自接项目（实模式）。

2. 发展期：公司慢慢做大，项目越接越多（保护模式、多进程），为了管理各个外包项目，建立了项目管理体系（进程管理）、会议室管理体系（内存管理）、文档资料管理系统（文件系统）、售前售后体系（输入输出设备管理）。

3. 壮大期：公司越来越牛，开始促进内部项目的合作（进程间通信）和外部公司合作（网络通信）。

4. 集团化：公司的业务越来越多，会成立多家子公司（虚拟化），或者鼓励内部创业（容器化），这个时候公司就变成了集团。大管家的调度能力不再局限于一家公司，而是集团公司（Linux 集群），从而成功上市（从单机操作系统到数据中心操作系统）。


#### 学习路径（要爬的六个坡）

1. 第一个坡：抛弃旧的思维习惯，熟练使用 Linux 命令行

2. 第二个坡：通过系统调用或者 glibc，学会自己进行程序设计

3. 第三个坡：了解 Linux 内核机制，反复研习重点突破

4. 第四个坡：阅读 Linux 内核代码，聚焦核心逻辑和场景

5. 第五个坡：实验定制化 Linux 组件，已经没人能阻挡你成为内核开发工程师了

6. 最后一个坡：面向真实场景的开发，实践没有终点



## # linux操作系统综述

#### 快速上手几个命令

在 Linux 下面，凭借rpm -qa和dpkg -l就可以查看安装的软件列表，-q 就是 query，a 就是 all，-l 的意思就是 list。如果真的去运行的话，你会发现这个列表很长很长，很难找到你安装的软件。如果你知道要安装的软件包含某个关键词，可以用一个很好用的搜索工具 grep。

rpm -qa | grep jdk，这个命令是将列出来的所有软件形成一个输出。| 是管道，用于连接两个程序，前面 rpm -qa 的输出就放进管道里面，然后作为 grep 的输入，grep 将在里面进行搜索带关键词 jdk 的行，并且输出出来。grep 支持正则表达式，因此搜索的时候很灵活，再加上管道，这是一个很常用的模式。同理dpkg -l | grep jdk也是能够找到的。

如果你不知道关键词，可以使用rpm -qa | more和rpm -qa | less这两个命令，它们可以将很长的结果分页展示出来。这样你就可以一个个来找了。我们还是利用管道的机制。more 是分页后只能往后翻页，翻到最后一页自动结束返回命令行，less 是往前往后都能翻页，需要输入 q 返回命令行，q 就是 quit。

Windows 上有了软件管家，Linux 也有自己的软件管家，CentOS 下面是 yum，Ubuntu 下面是 apt-get。

你可以根据关键词搜索，例如搜索jdk、yum search jdk和apt-cache search jdk，可以搜索出很多很多可以安装的 jdk 版本。如果数目太多，你可以通过管道 grep、more、less 来进行过滤。选中一个之后，我们就可以进行安装了。你可以用yum install java-11-openjdk.x86_64和apt-get install openjdk-9-jdk来进行安装。

其实无论是先下载再安装，还是通过软件管家进行安装，都是下载一些文件，然后将这些文件放在某个路径下，然后在相应的配置文件中配置一下。例如，在 Windows 里面，最终会变成 C:\Program Files 下面的一个文件夹以及注册表里面的一些配置。对应 Linux 里面会放的更散一点。例如，主执行文件会放在 /usr/bin 或者 /usr/sbin 下面，其他的库文件会放在 /var 下面，配置文件会放在 /etc 下面。

如果是 tar.gz 这种格式的，通过 tar xvzf jdk-XXX_linux-x64_bin.tar.gz 就可以解压缩了，对于 Windows 上，如果采取这种下载压缩包的格式，需要在系统设置的环境变量配置里面设置PATH。在 Linux 也是一样的，通过 tar 解压缩之后，也需要配置环境变量，可以通过 export 命令来配置。export 命令仅在当前命令行的会话中管用，一旦退出重新登录进来，就不管用了。

在当前用户的默认工作目录，例如 /root 或者 /home/cliu8 下面，有一个.bashrc 文件，这个文件是以点开头的，这个文件默认看不到，需要 ls -la 才能看到，a 就是 all。每次登录的时候，这个文件都会运行，因而把它放在这里。这样登录进来就会自动执行。当然也可以通过 source .bashrc 手动执行。

我们都知道 Windows 下的程序，如果后缀名是 exe，双击就可以运行了。Linux 不是根据后缀名来执行的。它的执行条件是这样的：只要文件有 x 执行权限，都能到文件所在的目录下，通过./filename运行这个程序。当然，如果放在 PATH 里设置的路径下面，就不用./ 了，直接输入文件名就可以运行了，Linux 会帮你找。

进程如何关闭？ 
ps -ef |grep 关键字  |awk '{print $2}'|xargs kill -9
awk 工具可以很灵活地对文本进行处理，这里的 awk '{print $2}'是指第二列的内容，是运行的程序 ID。我们可以通过 xargs 传递给 kill -9，也就是发给这个运行的程序一个信号，让它关闭。如果你已经知道运行的程序 ID，可以直接使用 kill 关闭运行的程序。其中 ps -ef 可以单独执行，列出所有正在运行的程序，grep 上面我们介绍过了，通过关键字找到咱们刚才启动的程序。


* linux程序运行方式
     1. 这是 Linux 执行程序最常用的一种方式，通过 shell 在交互命令行里面运行
     2. Linux 运行程序的第二种方式，后台运行。我们往往使用nohup命令。这个命令的意思是 no hang up（不挂起），也就是说，当前交互命令行退出的时候，程序还要在。当然这个时候，程序不能霸占交互命令行，而是应该在后台运行。最后加一个 &，就表示后台运行。
     3. 在 Windows 里面还有一种程序，称为服务。这是系统启动的时候就在的，我们可以通过控制面板的服务管理启动和关闭它。Linux 也有相应的服务，这就是程序运行的第三种方式，以服务的方式运行。例如常用的数据库 MySQL，就可以使用这种方式运行。例如：在 CentOS 里有些特殊，MySQL 被 Oracle 收购后，因为担心授权问题，改为使用 MariaDB，它是 MySQL 的一个分支。通过命令yum install mariadb-server mariadb进行安装，命令systemctl start mariadb启动，命令systemctl enable mariadb设置开机启动。同理，会在 /usr/lib/systemd/system 目录下，创建一个 XXX.service 的配置文件，从而成为一个服务。


#### 学会几个系统调用

1. fork
在 Linux 里，要创建一个新的进程，需要一个老的进程调用 fork 来实现，其中老的进程叫作父进程（Parent Process），新的进程叫作子进程（Child Process）。启动的时候先创建一个所有用户进程的“祖宗进程”。

当父进程调用 fork 创建进程的时候，子进程将各个子系统为父进程创建的数据结构也全部拷贝了一份，甚至连程序代码也是拷贝过来的。按理说，如果不进行特殊的处理，父进程和子进程都按相同的程序代码进行下去，这样就没有意义了。所以，我们往往会这样处理：对于 fork 系统调用的返回值，如果当前进程是子进程，就返回 0；如果当前进程是父进程，就返回子进程的进程号。这样首先在返回值这里就有了一个区分，然后通过 if-else 语句判断，如果是父进程，还接着做原来应该做的事情；如果是子进程，需要请求另一个系统调用execve来执行另一个程序，这个时候，子进程和父进程就彻底分道扬镳了，也就产生了一个分支（fork）了。

有时候，父进程要关心子进程的运行情况，这毕竟是自己身上掉下来的肉。有个系统调用waitpid，父进程可以调用它，将子进程的进程号作为参数传给它，这样父进程就知道子进程运行完了没有，成功与否。

2. 在堆里面分配内存的系统调用，brk和mmap。
当分配的内存数量比较小的时候，使用 brk，会和原来的堆的数据连在一起，这就像多分配两三个工位，在原来的区域旁边搬两把椅子就行了。当分配的内存数量比较大的时候，使用 mmap，会重新划分一块区域，也就是说，当办公空间需要太多的时候，索性来个一整块。

在操作系统中，每个进程都有自己的内存，互相之间不干扰，有独立的进程内存空间。

对于进程的内存空间来讲，放程序代码的这部分，我们称为代码段（Code Segment）。

对于进程的内存空间来讲，放进程运行中产生数据的这部分，我们称为数据段（Data Segment）。其中局部变量的部分，在当前函数执行的时候起作用，当进入另一个函数时，这个变量就释放了；也有动态分配的，会较长时间保存，指明才销毁的，这部分称为堆（Heap）。

3. 文件管理
对于文件的操作，下面这六个系统调用是最重要的：对于已经有的文件，可以使用open打开这个文件，close关闭这个文件；对于没有的文件，可以使用creat创建文件；打开文件以后，可以使用lseek跳到文件的某个位置；可以对文件的内容进行读写，读的系统调用是read，写是write。

4. Linux 里有一个特点，那就是一切皆文件。
     * 启动一个进程，需要一个程序文件，这是一个二进制文件。
     * 启动的时候，要加载一些配置文件，例如 yml、properties 等，这是文本文件；启动之后会打印一些日志，如果写到硬盘上，也是文本文件。
     * 但是如果我想把日志打印到交互控制台上，在命令行上唰唰地打印出来，这其实也是一个文件，是标准输出 stdout 文件。
     * 这个进程的输出可以作为另一个进程的输入，这种方式称为管道，管道也是一个文件。
     * 进程可以通过网络和其他进程进行通信，建立的 Socket，也是一个文件。
     * 进程需要访问外部设备，设备也是一个文件。
     * 文件都被存储在文件夹里面，其实文件夹也是一个文件。
     * 进程运行起来，要想看到进程运行的情况，会在 /proc 下面有对应的进程号，还是一系列文件。

每个文件，Linux 都会分配一个文件描述符（File Descriptor），这是一个整数。有了这个文件描述符，我们就可以使用系统调用，查看或者干预进程运行的方方面面。所以说，文件操作是贯穿始终的，这也是“一切皆文件”的优势，就是统一了操作的入口，提供了极大的便利。


5. 信号处理（经常遇到的信号有以下几种）
     * 在执行一个程序的时候，在键盘输入“CTRL+C”，这就是中断的信号，正在执行的命令就会中止退出；
     * 如果非法访问内存；
     * 硬件故障，设备出了问题；
     * 用户进程通过kill函数，将一个用户信号发送给另一个进程。

对于一些不严重的信号，可以忽略，该干啥干啥，但是像 SIGKILL（用于终止一个进程的信号）和 SIGSTOP（用于中止一个进程的信号）是不能忽略的，可以执行对于该信号的默认动作。每种信号都定义了默认的动作，例如硬件故障，默认终止；**也可以提供信号处理函数，可以通过sigaction系统调用，注册一个信号处理函数。**提供了信号处理服务，项目执行过程中一旦有变动，就可以及时处理了。

6. 进程间通信方式
     * 消息队列: 这个消息队列是在内核里的，我们可以通过msgget创建一个新的队列，msgsnd将消息发送到消息队列，而消息接收方可以使用msgrcv从队列中取消息。
     * 共享内存: 这样数据就不需要拷贝来拷贝去，我们可以通过shmget创建一个共享内存块，通过shmat将共享内存映射到自己的内存空间，然后就可以读写了。
     * 信号量:   同时修改同一块数据咋办？这就需要有一种方式，让不同的人能够排他地访问，这就是信号量的机制 Semaphore。这个机制比较复杂，这里说一种简单的场景: 对于只允许一个人访问的需求，我们可以将信号量设为 1。当一个人要访问的时候，先调用sem_wait。如果这时候没有人访问，则占用这个信号量，他就可以开始访问了。如果这个时候另一个人要访问，也会调用 sem_wait。由于前一个人已经在访问了，所以后面这个人就必须等待上一个人访问完之后才能访问。当上一个人访问完毕后，会调用sem_post将信号量释放，于是下一个人等待结束，可以访问这个资源了。

7. 网络通信
不同机器的通过网络相互通信，要遵循相同的网络协议，也即 TCP/IP 网络协议栈。Linux 内核里有对于网络协议栈的实现。如何暴露出服务给项目组使用呢？

网络服务是通过套接字 Socket 来提供服务的。Socket 这个名字很有意思，可以作“插口”或者“插槽”讲。虽然我们是写软件程序，但是你可以想象成弄一根网线，一头插在客户端，一头插在服务端，然后进行通信。因此，在通信之前，双方都要建立一个 Socket。

我们可以通过 Socket 系统调用建立一个 Socket。Socket 也是一个文件，也有一个文件描述符，也可以通过读写函数进行通信。

8. glibc
平时并没有直接使用系统调用。为了对用户更友好，我们还可以使用 Glibc，它会转换成为系统调用，帮你调用。Glibc 是 Linux 下使用的开源的标准 C 库，它是 GNU 发布的 libc 库。Glibc 为程序员提供丰富的 API，除了例如字符串处理、数学运算等用户态服务之外，最重要的是封装了操作系统提供的系统服务，即系统调用的封装。

每个特定的系统调用对应了至少一个 Glibc 封装的库函数，比如说，系统提供的打开文件系统调用 sys_open 对应的是 Glibc 中的 open 函数。

有时候，Glibc 一个单独的 API 可能调用多个系统调用，比如说，Glibc 提供的 printf 函数就会调用如 sys_open、sys_mmap、sys_write、sys_close 等等系统调用。

也有时候，多个 API 也可能只对应同一个系统调用，如 Glibc 下实现的 malloc、calloc、free 等函数用来分配和释放内存，都利用了内核的 sys_brk 的系统调用。


#### 实验
有个命令 strace，常用来跟踪进程执行时系统调用和所接收的信号。可以试一下平时使用的命令行，看看都执行了哪些系统调用。


#### 总结
一、 创建进程
##### 创建进程的总结：
1. Linux中父进程调用fork创建子进程。
2. 父进程调用fork时，子进程拷贝所有父进程的数据接口和代码过来。
3. 当前进程是子进程，fork返回0；当前进程是父进程，fork返回子进程进程号
4. 如果返回0，说明当前进程是子进程，子进程请求execve系统调用，执行另一个程序。
5. 如果返回子进程号，说明当前进程是父进程，按照原父进程原计划执行。
6. 父进程要对子进程负责，调用waitpid将子进程进程号作为参数，父进程就能知道子进程运行完了没有，成功与否。
7. 操作系统启动的时候先创建了一个所有用户进程的“祖宗进程”，课时1，第3题A选项：0号进程是所有用户态进程的祖先
##### 创建进程的系统调用：fork
##### 执行另一个程序的系统调用：execve
##### 将子进程的进程号作为参数传给它，父进程就能知道子进程运行完了没有，成功与否：waitpid

二、 内存管理
##### 内存管理总结
1. 每个进程都有独立的进程内存空间，互相之间不干扰。（隔离性）
2. 进程内存空间，存放程序代码的部分，称为代码段（Code Segment）。
3. 存放进程运行中产生数据的部分，称为数据段（Data Segment）。
4. 进程写入数据的时候，现用现分物理内存给进程使用。
5. 分配内存数量比较小时，使用brk调用，会和原来的堆数据连在一起。
6. 需要分配的内存数据量比较大的时候，使用mmap，重新划分一块内存区域。
##### 分配较小内存数量，和原来堆内数据连在一起：brk
##### 分配较大内存数量，重新划分一块内存区域：mmap

三、 文件管理
##### 文件的操作六个最重要系统调用：
##### 打开文件：open
##### 关闭文件：close
##### 创建文件：creat
##### 打开文件后跳到文件某个位置：lseek
##### 读文件：read
##### 写文件：write
##### Linux一切皆文件
##### 一切皆文件的优势即使统一了操作的入口，提供了极大的便利。

四、 信号处理（异常处理）
进程执行过程中一旦有变动，就可以通过信号处理服务及时处理。

五、 进程间通信
#### 有两种方式实现进程间通信
#### 消息队列方式
##### 创建一个新的队列：msgget
##### 发送消息到消息队列：msgsnd
##### 取出队列中的消息：msgrcv

六、 共享内存方式
##### 创建共享内存块：shmget
##### 将共享内存映射到自己的内存空间：shmat

#### 利用信号量实现隔离性
##### 占用信号量：sem_wait
##### 释放信号量：sem_post
伪代码：
假设信号量为1
```c++
signal = 1
// sem_wait伪代码
while True {
if sem_wait == 1；
    signal -=1;
    break;
}
code.code;
// sem_post伪代码
signal +=1;

```

七、 网络通信
##### 网络插口：socket
##### 网络通信遵循TCP/IP网络协议栈
##### 

八、 glibc
##### glibc是Linux下开源标准C库
##### glibc把系统调用进一步封

##### sys_open对应glibc的open函数
##### 一个单独的glibcAPI可能调用多个系统调用
##### printf函数调用sys_open、sys_mmap、sys_write、sys_close等等系统调用


## # 系统初始化-x86架构

#### 总结
- CPU 包括: 运算单元, 数据单元, 控制单元
    - 运算单元 不知道算哪些数据, 结果放哪
    - 数据单元 包括 CPU 内部缓存和寄存器, 暂时存放数据和结果
    - 控制单元 获取下一条指令, 指导运算单元取数据, 计算, 存放结果
- 进程包含代码段, 数据段等, 以下为 CPU 执行过程:
    - 控制单元 通过指令指针寄存器(IP), 取下一条指令, 放入指令寄存器中
        - 指令包括操作和目标数据
    - 数据单元 根据控制单元的指令, 从数据段读数据到数据寄存器中
    - 运算单元 开始计算, 结果暂时存放到数据寄存器
- 两个寄存器, 存当前进程代码段和数据段起始地址, 在进程间切换
- 总线包含两类数据: 地址总线和数据总线
---
- x86 开放, 统一, 兼容
- 数据单元 包含 8个 16位通用寄存器, 可分为 2个 8位使用
- 控制单元 包含 IP(指令指针寄存器) 以及 4个段寄存器 CS DS SS ES
    - IP 存放指令偏移量
    - 数据偏移量存放在通用寄存器中
    - `段地址<<4 + 偏移量` 得到地址
---
- 32 位处理器
- 通用寄存器 从 8个 16位拓展为 8个 32位, 保留 16位和 8位使用方式
- IP 从 16位扩展为 32位, 保持兼容
- 段寄存器仍为 16位, 由段描述符(表格, 缓存到 CPU 中)存储段的起始地址, 由段寄存器选择其中一项保证段地址灵活性与兼容性
---
- 16位为实模式, 32位为保护模式
- 刚开机为实模式, 需要更多内存切换到保护模式
---
- x86 有两种模式，一种模式是实模式，只能寻址 1M，每个段最多 64K。这个太小了。另一种是保护模式，即 32 位系统，能够寻址 4G。
---
原来 x86 架构是指 8086 ，而 x86 是代表 32 位操作系统是因为 80386，原来这两个 x86 不是同一个意思啊，以前学操作系统的时候一直想不明白 x86 为什么是指代 32 位操作系统


#### 推荐读物
《深入理解计算机系统》

#### 掌握汇编的几个常用指令
move a b :把b值赋给a,使a=b
call和ret :call调用子程序，子程序以ret结尾
jmp :无条件跳
int :中断指令
add a b : 加法,a=a+b
or :或运算
xor :异或运算
shl :算术左移
ahr :算术右移
push xxx :压xxx入栈
pop xxx: xxx出栈
inc: 加1
dec: 减1
sub a b : a=a-b
cmp: 减法比较，修改标志位


## # 系统初始化-从BIOS到bootloader
x86 有两种模式，一种模式是实模式，只能寻址 1M，每个段最多 64K。这个太小了。另一种是保护模式，即 32 位系统，能够寻址 4G。

#### BIOS 初始化阶段
在主板上，有一个东西叫 ROM（Read Only Memory，只读存储器）。这和平常说的内存 RAM（Random Access Memory，随机存取存储器）不同。平时买的内存条是可读可写的，这样才能保存计算结果。而 ROM 是只读的，上面早就固化了一些初始化的程序，也就是 BIOS（Basic Input and Output System，基本输入输出系统）。

在 x86 系统中，将 1M 空间最上面的 0xF0000 到 0xFFFFF 这 64K 映射给 ROM，也就是说，到这部分地址访问的时候，会访问 ROM。当电脑刚加电的时候，会做一些重置的工作，将 CS 设置为 0xFFFF，将 IP 设置为 0x0000，所以第一条指令就会指向 0xFFFF0，正是在 ROM 的范围内。在这里，有一个 JMP 命令会跳到 ROM 中做初始化工作的代码，于是，BIOS 开始进行初始化的工作。第一条，BIOS 要检查一下系统的硬件是不是都好着呢。

这个时候，要建立一个中断向量表和中断服务程序，因为现在你还要用键盘和鼠标，这些都要通过中断进行的。也要给客户输出一些结果，也就是在内存空间映射显存的空间，在显示器上显示一些字符。

#### bootloader 初始化阶段
操作系统在哪儿呢？一般都会在安装在硬盘上，在 BIOS 的界面上。你会看到一个启动盘的选项。启动盘有什么特点呢？它一般在第一个扇区，占 512 字节，而且以 0xAA55 结束。这是一个约定，当满足这个条件的时候，就说明这是一个启动盘，在 512 字节以内会启动相关的代码。

这些代码是谁放在这里的呢？在 Linux 里面有一个工具，叫 Grub2，全称 Grand Unified Bootloader Version 2。顾名思义，就是搞系统启动的。可以通过 grub2-mkconfig -o /boot/grub2/grub.cfg 来配置系统启动的选项。这里面的选项会在系统启动的时候，成为一个列表，让你选择从哪个系统启动。使用 grub2-install /dev/sda，可以将启动程序安装到相应的位置。


#### grub2安装操作系统过程 （grub2: 非常牛的 Linux 启动管理器）
grub2 第一个要安装的就是 boot.img。它由 boot.S 编译而成，一共 512 字节，正式安装到启动盘的第一个扇区。这个扇区通常称为 MBR（Master Boot Record，主引导记录 / 扇区）。

- BIOS 完成任务后，会将 boot.img 从硬盘加载到内存中的 0x7c00 来运行。
- boot.img 做不了太多的事情。它能做的最重要的一个事情就是加载 grub2 的另一个镜像 core.img。
- core.img 由 lzma_decompress.img、diskboot.img、kernel.img 和一系列的模块组成，功能比较丰富，能做很多事情。
- boot.img 先加载的是 core.img 的第一个扇区，这个扇区里面是 diskboot.img，对应的代码是 diskboot.S。
- boot.img 将控制权交给 diskboot.img 后，diskboot.img 的任务就是将 core.img 的其他部分加载进来，先是解压缩程序 lzma_decompress.img，再往下是 kernel.img，最后是各个模块 module 对应的映像。这里需要注意，它不是 Linux 的内核，而是 grub 的内核。
- lzma_decompress.img 对应的代码是 startup_raw.S，本来 kernel.img 是压缩过的，现在执行的时候，需要解压缩。
- 在这之前，我们所有遇到过的程序都非常非常小，完全可以在实模式下运行，但是随着我们加载的东西越来越大，实模式这 1M 的地址空间实在放不下了，所以在真正的解压缩之前，lzma_decompress.img 做了一个重要的决定，就是调用 real_to_prot，切换到保护模式，这样就能在更大的寻址空间里面，加载更多的东西。
---
- 切换到保护模式要干很多工作，大部分工作都与内存的访问方式有关。
- 第一项是启用分段，就是在内存里面建立段描述符表，将寄存器里面的段寄存器变成段选择子，指向某个段描述符，这样就能实现不同进程的切换了。第二项是启动分页。能够管理的内存变大了，就需要将内存分成相等大小的块，这些可以参考内存管理相关笔记。
- 在实模式 8086 下面，一共就 20 个地址线，可访问 1M 的地址空间。保护模式需要做一项工作，那就是打开 Gate A20，也就是第 21 根地址线的控制线。
- 有了空间了，接下来我们要对压缩过的 kernel.img 进行解压缩，然后跳转到 kernel.img 开始运行。
- kernel.img 对应的代码是 startup.S 以及一堆 c 文件，在 startup.S 中会调用 grub_main，这是 grub kernel 的主函数。在这个函数里面，grub_load_config() 开始解析，我们上面写的那个 grub.conf 文件里的配置信息。
- 如果是正常启动，grub_main 最后会调用 grub_command_execute (“normal”, 0, 0)，最终会调用 grub_normal_execute() 函数。在这个函数里面，grub_show_menu() 会显示出让你选择的那个操作系统的列表。
- 一旦，你选定了某一项，启动某个操作系统，就要开始调用，grub_menu_execute_entry() ，开始解析并执行你选择的那一项，正式开始安装操作系统。
- 例如里面的 linux16 命令，表示装载指定的内核文件，并传递内核启动参数。于是 grub_cmd_linux() 函数会被调用，它会首先读取 Linux 内核镜像头部的一些数据结构，放到内存中的数据结构来，进行检查。如果检查通过，则会读取整个 Linux 内核镜像到内存。
- 如果配置文件里面还有 initrd 命令，用于为即将启动的内核传递 init ramdisk 路径。于是 grub_cmd_initrd() 函数会被调用，将 initramfs 加载到内存中来。
- 当这些事情做完之后，grub_command_execute (“boot”, 0, 0) 才开始真正地启动内核。



#### 总结
- 实模式只有 1MB 内存寻址空间(X86)
- 加电, 重置 CS 为 0xFFFF , IP 为 0x0000, 对应 BIOS 程序
- 0xF0000-0xFFFFF 映射到 BIOS 程序(存储在ROM中), BIOS 做以下三件事:
    - 检查硬件
    - 提供基本输入(中断)输出(显存映射)服务
    - 加载 MBR 到内存(0x7c00)
- MRB: 启动盘第一个扇区(512B, 由 Grub2 写入 boot.img 镜像)
- boot.img 加载 Grub2 的 core.img 镜像
- core.img 包括 diskboot.img, lzma_decompress.img, kernel.img(这里需要注意，它不是 Linux 的内核，而是 grub 的内核) 以及其他模块
- boot.img 先加载运行 diskboot.img, 再由 diskboot.img 加载 core.img 的其他内容
- diskboot.img 解压运行 lzma_decompress.img, 由lzma_decompress.img 切换到保护模式
---
- 切换到保护模式需要做以下三件事:
    - 启用分段, 辅助进程管理
    - 启动分页, 辅助内存管理
    - 打开其他地址线
- lzma_decompress.img 解压运行 grub 内核 kernel.img, kernel.img 做以下四件事:
    - 解析 grub.conf 文件
    - 选择操作系统
    - 例如选择 linux16, 会先读取内核头部数据进行检查, 检查通过后加载完整系统内核
    - 启动系统内核


## # 系统初始化-内核初始化
内核的启动从入口函数 start_kernel() 开始。在 init/main.c 文件中，start_kernel 相当于内核的 main 函数。打开这个函数，你会发现，里面是各种各样初始化函数 XXXX_init。

#### Linux初始化
1. 进程管理初始化：
在操作系统里面，先要有个创始进程，有一行指令 set_task_stack_end_magic(&init_task)。这里面有一个参数 init_task，它的定义是 struct task_struct init_task = INIT_TASK(init_task)。它是系统创建的第一个进程，我们称为 0 号进程。这是唯一一个没有通过 fork 或者 kernel_thread 产生的进程，是进程列表的第一个。

2. 中断初始化（相当于办事大厅）: 
这里面对应的函数是 trap_init()，里面设置了很多中断门（Interrupt Gate），用于处理各种中断。其中有一个 set_system_intr_gate(IA32_SYSCALL_VECTOR, entry_INT80_32)，这是系统调用的中断门。系统调用也是通过发送中断的方式进行的。当然，64 位的有另外的系统调用方法，可查看系统调用相关笔记。

3. 内存管理模块初始化: 
对应的，mm_init()。

4. 调度模块初始化: 
进程运行需要执行一定的调度策略。sched_init() 就是用于初始化调度模块。

5. 内存文件系统初始化: 
vfs_caches_init() 会用来初始化基于内存的文件系统 rootfs。在这个函数里面，会调用 mnt_init()->init_rootfs()。这里面有一行代码，register_filesystem(&rootfs_fs_type)。在 VFS 虚拟文件系统里面注册了一种类型，我们定义为 struct file_system_type rootfs_fs_type。为了兼容各种各样的文件系统，需要将文件的相关数据结构和操作抽象出来，形成一个抽象层对上提供统一的接口，这个抽象层就是 VFS（Virtual File System），虚拟文件系统。

6. 初始化1号进程
rest_init函数的第一大工作是，用 kernel_thread(kernel_init, NULL, CLONE_FS) 创建第二个进程，这个是 1 号进程。
    - 1 号进程对于操作系统来讲，有“划时代”的意义。因为它将运行一个用户进程，进程树就是基于这个进程逐渐形成的。
    - 一旦有了用户进程，就要开始做一定的区分，哪些是核心资源，哪些是非核心资源，x86 提供了分层的权限机制，把区域分成了四个 Ring，越往里权限越高Ring0最高，越往外权限越低。
    - 操作系统很好地利用了这个机制，将能够访问关键资源的代码放在 Ring0，我们称为内核态（Kernel Mode）；将普通的程序代码放在 Ring3，我们称为用户态（User Mode）。
    - 系统已经处于保护模式了，保护模式除了可访问空间大一些，还有另一个重要功能，就是“保护”，也就是说，当处于用户态的代码想要执行更高权限的指令，这种行为是被禁止的，要防止它们为所欲为。
    - 如果用户态的代码想要访问核心资源，中断模块是统一的入口，用户态代码在这里请求就是了。用户态代码不用管内核态发生了什么，执行完了返回结果就可以了。

7. 从用户态到内核态的执行流程
当一个用户态的程序运行到一半，要访问一个核心资源，例如访问网卡发一个网络包，就需要暂停当前的运行，调用系统调用，接下来就轮到内核中的代码运行了。
    - 首先，内核将从系统调用传过来的包，在网卡上排队，轮到的时候就发送。发送完了，系统调用就结束了，返回用户态，让暂停运行的程序接着运行。
    - 暂停其实就是把程序运行到一半的情况保存下来。内存是用来保存程序运行时候的中间结果的，现在要暂时停下来，这些中间结果不能丢，因为再次运行的时候，还要基于这些中间结果接着来。另外就是，当前运行到代码的哪一行了，当前的栈在哪里，这些都是在寄存器里面的。
    - 所以，暂停的那一刻，要把当时 CPU 的寄存器的值全部暂存到一个地方，这个地方可以放在进程管理系统很容易获取的地方。当系统调用完毕，返回的时候，再从这个地方将寄存器的值恢复回去，就能接着运行了。
整个过程就是这样的：用户态 - 系统调用 - 保存寄存器 - 内核态执行系统调用 - 恢复寄存器 - 返回用户态，然后接着运行。

8. 从内核态到用户态的执行过程
1 号进程启动的过程中。当执行 kernel_thread 这个函数的时候，我们还在内核态，kernel_thread 的参数是一个函数 kernel_init，也就是这个进程会运行这个函数。在 kernel_init 里面，会调用 kernel_init_freeable()，源码说明1 号进程运行的是一个文件。如果打开 run_init_process 函数，会发现它调用的是 do_execve。execve 是一个系统调用，它的作用是运行一个执行文件。它会尝试运行 ramdisk 的“/init”，或者普通文件系统上的“/sbin/init”“/etc/init”“/bin/init”“/bin/sh”。不同版本的 Linux 会选择不同的文件启动，但是只要有一个起来了就可以。

要运行一个程序，需要加载这个二进制文件，它是有一定格式的。Linux 下一个常用的格式是 ELF（Executable and Linkable Format，可执行与可链接格式）。这其实就是先调用 load_elf_binary，最后调用 start_thread。

start_thread参数中的 struct pt_regs 看名字里的 register，就是寄存器，这个结构就是在系统调用的时候，内核中保存用户态运行上下文的，里面将用户态的代码段 CS 设置为 __USER_CS，将用户态的数据段 DS 设置为 __USER_DS，以及指令指针寄存器 IP、栈指针寄存器 SP。

start_thread函数最后调用的force_iret函数是用于从系统调用中返回。这个时候会恢复寄存器。CS 和指令指针寄存器 IP 恢复了，指向用户态下一个要执行的语句。DS 和函数栈指针 SP 也被恢复了，指向用户态函数栈的栈顶。所以，下一条指令，就从用户态开始运行了。

9. ramdisk 的作用
init 终于从内核到用户态了。一开始到用户态的是 ramdisk 的 init，后来会启动真正根文件系统上的 init，成为所有用户态进程的祖先。

是因为刚才那个 init 程序是在文件系统上的，文件系统一定是在一个存储设备上的，例如硬盘。Linux 访问存储设备，要有驱动才能访问。如果存储系统数目很有限，那驱动可以直接放到内核里面，反正前面我们加载过内核到内存里了，现在可以直接对存储系统进行访问。但是存储系统越来越多了，如果所有市面上的存储系统的驱动都默认放进内核，内核就太大了。这该怎么办呢？

只好先弄一个基于内存的文件系统。内存访问是不需要驱动的，这个就是 ramdisk。这个时候，ramdisk 是根文件系统。

然后，我们开始运行 ramdisk 上的 /init。等它运行完了就已经在用户态了。/init 这个程序会先根据存储系统的类型加载驱动，有了驱动就可以设置真正的根文件系统了。有了真正的根文件系统，ramdisk 上的 /init 会启动文件系统上的 init。接下来就是各种系统的初始化。启动系统的服务，启动控制台，用户就可以登录进来了。

到这里rest_init 的第一个大事情就完成。仅仅形成了用户态所有进程的祖先。


10. 创建 2 号进程
用户态的所有进程都有大师兄了，也就是1号进程，那内核态的进程有没有一个进程统一管起来呢？有的，rest_init 第二大事情就是第三个进程，就是 2 号进程。

kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES) 又一次使用 kernel_thread 函数创建进程。这里需要指出一点，函数名 thread 可以翻译成“线程”，这也是操作系统很重要的一个概念。它和进程有什么区别呢？为什么这里创建的是进程，函数名却是线程呢？

从内核态来看，无论是进程，还是线程，我们都可以统称为任务（Task），都使用相同的数据结构，平放在同一个链表中。这里的函数 kthreadd，负责所有内核态的线程的调度和管理，是内核态所有线程运行的祖先。

#### 总结
- 内核初始化, 运行 `start_kernel()` 函数(位于 init/main.c), 初始化做三件事
    - 创建样板进程, 及各个模块初始化
    - 创建管理/创建用户态进程的进程
    - 创建管理/创建内核态进程的进程
---
- 创建样板进程,及各个模块初始化
    - 创建第一个进程, 0号进程. `set_task_stack_end_magic(&init_task)` and `struct task_struct init_task = INIT_TASK(init_task)`
    - 初始化中断, `trap_init()`. 系统调用也是通过发送中断进行, 由 `set_system_intr_gate()` 完成.
    - 初始化内存管理模块, `mm_init()`
    - 初始化进程调度模块, `sched_init()`
    - 初始化基于内存的文件系统 rootfs, `vfs_caches_init()`
        - VFS(虚拟文件系统)将各种文件系统抽象成统一接口
    - 调用 `rest_init()` 完成其他初始化工作
---
- 创建管理/创建用户态进程的进程, 1号进程
    - `rest_init()` 通过 `kernel_thread(kernel_init,...)` 创建 1号进程(工作在用户态).
    - 权限管理
        - x86 提供 4个 Ring 分层权限
        - 操作系统利用: Ring0-内核态(访问核心资源); Ring3-用户态(普通程序)
    - 用户态调用系统调用: 用户态-系统调用-保存寄存器-内核态执行系统调用-恢复寄存器-返回用户态
    - 新进程执行 kernel_init 函数, 先运行 ramdisk 的 /init 程序(位于内存中)
        - 首先加载 ELF 文件
        - 设置用于保存用户态寄存器的结构体
        - 返回进入用户态
        - /init 加载存储设备的驱动
     - kernel_init 函数启动存储设备文件系统上的 init
---
- 创建管理/创建内核态进程的进程, 2号进程
    - `rest_init()` 通过 `kernel_thread(kthreadd,...)` 创建 2号进程(工作在内核态).
    - `kthreadd` 负责所有内核态线程的调度和管理


## # 系统初始化-系统调用（中断，类似办事大厅）
站在系统调用的角度，层层深入下去，就能从某个系统调用的场景出发，了解内核中各个模块的实现机制。

Linux 提供了 glibc 这个中介。它更熟悉系统调用的细节，并且可以封装成更加友好的接口。你可以直接用。

#### glibc 对系统调用的封装
以最常用的系统调用 open，打开一个文件为线索，看看系统调用是怎么实现的。仅仅会解析到从 glibc 如何调用到内核的 open，至于 open 怎么实现，怎么打开一个文件，参考文件系统相关笔记。

为了方便，大部分用户会选择使用glibc，也就是说，调用的是 glibc 里面的 open 函数。这个函数是如何定义的呢？ ` int open(const char *pathname, int flags, mode_t mode) `。在 glibc 的源代码中，有个文件 syscalls.list，里面列着所有 glibc 的函数对应的系统调用。

另外，glibc 还有一个脚本 make-syscall.sh，可以根据上面的配置文件，对于每一个封装好的系统调用，生成一个文件。这个文件里面定义了一些宏，例如 #define SYSCALL_NAME open。

glibc 还有一个文件 syscall-template.S，使用上面这个宏，定义了这个系统调用的调用方式。

```c
T_PSEUDO (SYSCALL_SYMBOL, SYSCALL_NAME, SYSCALL_NARGS)
    ret
T_PSEUDO_END (SYSCALL_SYMBOL)

#define T_PSEUDO(SYMBOL, NAME, N)    PSEUDO (SYMBOL, NAME, N)
```

PSEUDO 也是一个宏，里面对于任何一个系统调用，会调用 DO_CALL。这也是一个宏，这个宏 32 位和 64 位的定义是不一样的。


#### 32 位系统调用过程
i386 目录下的 sysdep.h 文件，这里，将请求参数放在寄存器里面，根据系统调用的名称，得到系统调用号，放在寄存器 eax 里面，然后执行 ENTER_KERNEL。

这里面的 ENTER_KERNEL 是什么呢？`# define ENTER_KERNEL int $0x80`
int $0x80 就是触发一个软中断，通过它就可以陷入（trap）内核。


#### 64 位系统调用过程


#### 系统调用表
上面的系统调用的方式，都是最终到了系统调用表，但是到底调用内核的什么函数呢？系统调用表 sys_call_table 是怎么形成的呢？

- 32 位的系统调用表定义在 arch/x86/entry/syscalls/syscall_32.tbl 文件里。例如 open 是这样定义的：` 5  i386  open      sys_open  compat_sys_open `
- 64 位的系统调用定义在另一个文件 arch/x86/entry/syscalls/syscall_64.tbl 里。例如 open 是这样定义的：` 2  common  open      sys_open `
- 第一列的数字是系统调用号。可以看出，32 位和 64 位的系统调用号是不一样的。第三列是系统调用的名字，第四列是系统调用在内核的实现函数。它们都是以 sys_ 开头。
- 系统调用在内核中的实现函数要有一个声明。声明往往在 include/linux/syscalls.h 文件中。例如 sys_open 是这样声明的：`asmlinkage long sys_open(const char __user *filename, int flags, umode_t mode);`，真正的实现这个系统调用，一般在一个.c 文件里面，例如 sys_open 的实现在 fs/open.c 里面
- 声明和实现都好了。接下来，在编译的过程中，需要根据 syscall_32.tbl 和 syscall_64.tbl 生成自己的 unistd_32.h 和 unistd_64.h。生成方式在 arch/x86/entry/syscalls/Makefile 中。
- 这里面会使用两个脚本，其中第一个脚本 arch/x86/entry/syscalls/syscallhdr.sh，会在文件中生成 `#define __NR_open`；第二个脚本 arch/x86/entry/syscalls/syscalltbl.sh，会在文件中生成 `__SYSCALL(__NR_open, sys_open)`。这样，unistd_32.h 和 unistd_64.h 是对应的系统调用号和系统调用实现函数之间的对应关系。
- 在文件 arch/x86/entry/syscall_32.c，定义了这样一个表，里面 include 了这个头文件，从而所有的 sys_ 系统调用都在这个表里面了。
- 同理，在文件 arch/x86/entry/syscall_64.c，定义了这样一个表，里面 include 了这个头文件，这样所有的 sys_ 系统调用就都在这个表里面了。


#### 总结

glibc 将系统调用封装成更友好的接口， 本节解析 glibc 函数如何调用到内核的 open

---
- 用户进程调用 open 函数
    - glibc 的 syscal.list 列出 glibc 函数对应的系统调用
    - glibc 的脚本 make_syscall.sh 根据 syscal.list 生成对应的宏定义(函数映射到系统调用)
    - glibc 的 syscal-template.S 使用这些宏, 定义了系统调用的调用方式(也是通过宏)
    - 其中会调用 DO_CALL (也是一个宏), 32位与 64位实现不同
---
- 32位 DO_CALL (位于 i386 目录下 sysdep.h)
    - 将调用参数放入寄存器中, 由系统调用名得到系统调用号, 放入 eax
    - 执行 ENTER_KERNEL(一个宏), 对应 int $0x80 触发软中断, 进入内核
    - 调用软中断处理函数 entry_INT80_32(内核启动时, 由 trap_init() 配置)
    - entry_INT80_32 将用户态寄存器存入 pt_regs 中(保存现场以及系统调用参数), 调用 do_syscall_32_iraq_on 
    - do_syscall_32_iraq_on 从 pt_regs 中取系统调用号(eax), 从系统调用表得到对应实现函数, 取 pt_regs 中存储的参数, 调用系统调用
    - entry_INT80_32 调用 INTERRUPT_RUTURN(一个宏)对应 iret 指令, 系统调用结果存在 pt_regs 的 eax 位置, 根据 pt_regs 恢复用户态进程
---
- 64位 DO_CALL (位于 x86_64 目录下 sysdep.h)
    - 通过系统调用名得到系统调用号, 存入 rax; 不同中断, 执行 syscall 指令
    - MSR(特殊模块寄存器), 辅助完成某些功能(包括系统调用)
    - trap_init() 会调用 cpu_init->syscall_init 设置该寄存器
    - syscall 从 MSR 寄存器中, 拿出函数地址进行调用, 即调用 entry_SYSCALL_64
    - entry_SYSCALL_64 先保存用户态寄存器到 pt_regs 中
    - 调用 entry_SYSCALL64_slow_pat->do_syscall_64
    - do_syscall_64 从 rax 取系统调用号, 从系统调用表得到对应实现函数, 取 pt_regs 中存储的参数, 调用系统调用
    - 返回执行 USERGS_SYSRET64(一个宏), 对应执行 swapgs 和 sysretq 指令; 系统调用结果存在 pt_regs 的 ax 位置, 根据 pt_regs 恢复用户态进程
---
- 系统调用表 sys_call_table
    - 32位 定义在 arch/x86/entry/syscalls/syscall_32.tbl 
    - 64位 定义在 arch/x86/entry/syscalls/syscall_64.tbl
    - syscall_*.tbl 内容包括: 系统调用号, 系统调用名, 内核实现函数名(以 sys 开头)
    - 内核实现函数的声明: include/linux/syscall.h
    - 内核实现函数的实现: 某个 .c 文件, 例如 sys_open 的实现在 fs/open.c
        - .c 文件中, 以宏的方式替代函数名, 用多层宏构建函数头
    - 编译过程中, 通过 syscall_*.tbl 生成 unistd_*.h 文件
        - unistd_*.h 包含系统调用与实现函数的对应关系
    - syscall_*.h include 了 unistd_*.h 头文件, 并定义了系统调用表(数组)


## # 进程管理-进程

#### 进行编译：程序的二进制格式

- 在 Linux 下面，二进制的程序也要有严格的格式，这个格式我们称为 ELF（Executeable and Linkable Format，可执行与可链接格式）。这个格式可以根据编译的结果不同，分为不同的格式。
---
#### ELF的三种格式--可重定位文件（.o文件）
- 在编译的时候，先做预处理工作，例如将头文件嵌入到正文中，将定义的宏展开，然后就是真正的编译过程，最终编译成为.o 文件，这就是 ELF 的第一种类型，可重定位文件，结构：
    - ELF Header 文件的头是用于描述整个文件的。这个文件格式在内核中有定义，分别为 struct elf32_hdr 和 struct elf64_hdr。
    - .text: 放编译好的二进制可执行代码
    - .data：已经初始化好的全局变量
    - .rodata：只读数据，例如字符串常量、const 的变量
    - .bss：未初始化全局变量，运行时会置 0
    - .symtab：符号表，记录的则是函数和变量
    - .strtab：字符串表、字符串常量和变量名

- 为啥叫可重定位呢？我们可以想象一下，这个编译好的代码和变量，将来加载到内存里面的时候，都是要加载到一定位置的。比如说，调用一个函数，其实就是跳到这个函数所在的代码位置执行；再比如修改一个全局变量，也是要到变量的位置那里去修改。但是现在这个时候，还是.o 文件，不是一个可以直接运行的程序，这里面只是部分代码片段。
- 编译后的函数，将来被谁调用，在哪里调用都不清楚，就更别提确定位置了。所以，.o 里面的位置是不确定的，但是必须是可重新定位的，因为它将来是要做函数库的嘛，就是一块砖，哪里需要哪里搬，搬到哪里就重新定位这些代码、变量的位置。
- 要想让.o里的函数作为库文件被重用，不能以.o 的形式存在，而是要形成库文件，最简单的类型是静态链接库.a 文件（Archives），仅仅将一系列对象文件（.o）归档为一个文件，使用命令 ar 创建（如: `ar cr libstaticprocess.a process.o`）。
- 虽然这里 libstaticprocess.a 里面只有一个.o，但是实际情况可以有多个.o。当有程序要使用这个静态连接库的时候，会将.o 文件提取出来，链接到程序中。
- `gcc -o staticcreateprocess createprocess.o -L. -lstaticprocess`在这个命令里，-L 表示在当前目录下找.a 文件，-lstaticprocess 会自动补全文件名，比如加前缀 lib，后缀.a，变成 libstaticprocess.a，找到这个.a 文件后，将里面的 process.o 取出来，和 createprocess.o 做一个链接，形成二进制执行文件 staticcreateprocess。这个链接的过程，重定位就起作用了，原来 createprocess.o 里面调用了 create_process 函数，但是不能确定位置，现在将 process.o 合并了进来，就知道位置了。

---

#### ELF的三种格式--可执行文件
- 这个格式和.o 文件大致相似，还是分成一个个的 section，并且被节头表描述。只不过这些 section 是多个.o 文件合并过的。

- 格式
    - ELF Header
    - Segment Header Table
    - 代码段：.text、.rodata
    - 数据段: .data、.bss
    - 不加载到内存: .symtab、.strtab、Section Header Table

- 但是这个时候，这个文件已经是马上就可以加载到内存里面执行的文件了，因而这些 section 被分成了需要加载到内存里面的代码段、数据段和不需要加载到内存里面的部分，将小的 section 合成了大的段 segment，并且在最前面加一个段头表（Segment Header Table）。在代码里面的定义为 struct elf32_phdr 和 struct elf64_phdr，这里面除了有对于段的描述之外，最重要的是 p_vaddr，这个是这个段加载到内存的虚拟地址。
- 在 ELF 头里面，有一项 e_entry，也是个虚拟地址，是这个程序运行的入口。
- 静态链接库一旦链接进去，代码和变量的 section 都合并了，因而程序运行的时候，就不依赖于这个库是否存在。但是这样有一个缺点，就是相同的代码段，如果被多个程序使用的话，在内存里面就有多份，而且一旦静态链接库更新了，如果二进制执行文件不重新编译，也不随着更新。

#### ELF的三种格式--共享对象文件（动态链接库）
- 因为静态链接库的特点，又出现了另一种，动态链接库（Shared Libraries），不仅仅是一组对象文件的简单归档，而是多个对象文件的重新组合，可被多个程序共享。
- 当一个动态链接库被链接到一个程序文件中的时候，最后的程序文件并不包括动态链接库中的代码，而仅仅包括对动态链接库的引用，并且不保存动态链接库的全路径，仅仅保存动态链接库的名称。
- 当运行这个程序的时候，首先寻找动态链接库，然后加载它。默认情况下，系统在 /lib 和 /usr/lib 文件夹下寻找动态链接库。如果找不到就会报错，我们可以设定 LD_LIBRARY_PATH 环境变量，程序运行时会在此环境变量指定的文件夹下寻找动态链接库。
- 基于动态链接库创建出来的二进制文件格式还是 ELF，但是稍有不同。首先，多了一个.interp 的 Segment，这里面是 ld-linux.so，这是动态链接器，也就是说，运行时的链接动作都是它做的。
- 另外，ELF 文件中还多了两个 section，一个是.plt，过程链接表（Procedure Linkage Table，PLT），一个是.got.plt，全局偏移量表（Global Offset Table，GOT）。
- dynamiccreateprocess 这个程序如果要调用 libdynamicprocess.so 里的 create_process 函数。由于是运行时才去找，编译的时候，压根不知道这个函数在哪里，所以就在 PLT 里面建立一项 PLT[x]。这一项也是一些代码，有点像一个本地的代理，在二进制程序里面，不直接调用 create_process 函数，而是调用 PLT[x]里面的代理代码，这个代理代码会在运行的时候找真正的 create_process 函数。
- 去哪里找代理代码呢？这就用到了 GOT，这里面也会为 create_process 函数创建一项 GOT[y]。这一项是运行时 create_process 函数在内存中真正的地址。如果这个地址在 dynamiccreateprocess 调用 PLT[x]里面的代理代码，代理代码调用 GOT 表中对应项 GOT[y]，调用的就是加载到内存中的 libdynamicprocess.so 里面的 create_process 函数了。
- 但是 GOT 怎么知道的呢？对于 create_process 函数，GOT 一开始就会创建一项 GOT[y]，但是这里面没有真正的地址，因为它也不知道，但是它有办法，它又回调 PLT，告诉它，你里面的代理代码来找我要 create_process 函数的真实地址，我不知道，你想想办法吧。
- PLT 这个时候会转而调用 PLT[0]，也即第一项，PLT[0]转而调用 GOT[2]，这里面是 ld-linux.so 的入口函数，这个函数会找到加载到内存中的 libdynamicprocess.so 里面的 create_process 函数的地址，然后把这个地址放在 GOT[y]里面。下次，PLT[x]的代理函数就能够直接调用了。

#### 运行程序为进程
- 知道了 ELF 这个格式，这个时候它还是个程序，那怎么把这个文件加载到内存里面呢？在内核中，有这样一个数据结构（struct linux_binfmt），用来定义加载二进制文件的方法。
- 对于 ELF 文件格式，有对应的实现：static struct linux_binfmt elf_format。
- 观察这两种结构，发现和加载内核镜像的时候一样都是load_elf_binary函数
- 具体原理是 exec 这个系统调用最终调用的 load_elf_binary。exec 比较特殊，它是一组函数：
    - 包含 p 的函数（execvp, execlp）会在 PATH 路径下面寻找程序；
    - 不包含 p 的函数需要输入程序的全路径；
    - 包含 v 的函数（execv, execvp, execve）以数组的形式接收参数；
    - 包含 l 的函数（execl, execlp, execle）以列表的形式接收参数；
    - 包含 e 的函数（execve, execle）以数组的形式接收环境变量


#### 进程树
既然所有的进程都是从父进程 fork 过来的，那总归有一个祖宗进程，这就是系统启动的 init 进程。

- 在解析 Linux 的启动过程的时候，1 号进程是 /sbin/init。如果在 centOS 7 里面，我们 ls 一下，可以看到，这个进程是被软链接到 systemd 的。`/sbin/init -> ../lib/systemd/systemd`
- 系统启动之后，init 进程会启动很多的 daemon 进程，为系统运行提供服务，然后就是启动 getty，让用户登录，登录后运行 shell，用户启动的进程都是通过 shell 运行的，从而形成了一棵进程树。
- 通过 ps -ef 命令查看进程树：
    - 我们可以通过 ps -ef 命令查看当前系统启动的进程，我们会发现有三类进程。
        - 你会发现，PID 1 的进程就是 init 进程 systemd，PID 2 的进程是内核线程 kthreadd，这两个在内核启动的时候都见过。
        - 其中用户态的不带中括号。
        - 内核态的带中括号。
    - 接下来进程号依次增大，但是你会看所有带中括号的内核态的进程，祖先都是 2 号进程。而用户态的进程，祖先都是 1 号进程。tty 那一列，是问号的，说明不是前台启动的，一般都是后台的服务。
    - pts 的父进程是 sshd，bash 的父进程是 pts，ps -ef 这个命令的父进程是 bash。这样整个链条都比较清晰了。


#### 总结

- 写代码
- 编译成 ELF 格式的二进制文件, 有三种格式(可重定位 .o 文件; 可执行文件; 共享对象文件 .so)
- 可重定位 .o 文件(ELF 第一种格式)
    - .h + .c 文件, 编译得到**可重定位** .o 文件 
    - .o 文件由: ELF 头, 多个节(section), 节头部表组成(每个节有一项纪录); 节表的位置和纪录数由 ELF 头给出.
    - .o 文件只是程序部分代码片段
    - .rel.text 和 .rel.data 标注了哪些函数/数据需要重定位
    - 要函数可被调用, 要以库文件的形式存在, 最简单是创建静态链接库 .a 文件(Archives)
    - 通过 ar 创建静态链接库, 通过 gcc 提取库文件中的 .o 文件, 链接到程序中
    - 链接合并后, 就可以定位到函数/数据的位置, 形成可执行文件
- 可执行文件(ELF 第二种格式)
    - 链接合并后, 形成可执行文件
    - 同样包含: ELF 头, 多个节, 节头部表; 另外还有段头表(包含段的描述, p_vaddr 段加载到内存的虚拟地址)
    - ELF 头中有 e_entry , 指向程序入口的虚拟地址
- 共享对象 .so 文件(ELF 第三种格式)
    - 静态链接库合并进可执行文件, 多个进程不能共享
    - 动态链接库-链接了动态链接库的程序, 仅包含对该库的引用(且只保存名称)
    - 通过 gcc 创建, 通过 gcc 链接
    - 运行时, 先找到动态链接库(默认在 /lib 和 /usr/lib 找)
    - 增加了 .interp 段, 里面是 ld_linux.so (动态链接器)
    - 增加了两个节 .plt(过程链接表)和 .got.plt(全局偏移表)
    - 一个动态链接函数对应 plt 中的一项 plt[x], plt[x] 中是代理代码, 调用 got 中的一项 got[y]
    - 起始, got 没有动态链接函数的地址, 都指向 plt[0], plt[0] 又调用 got[2], got[2]指向 ld_linux.so
    - ld_linux.so 找到加载到内存的动态链接函数的地址, 并将地址存入 got[y]
- 加载 ELF 文件到内存
    - 通过系统调用 exec 调用 load_elf_binary
    - exec 是一组函数
        - 包含 p: 在 PATH 中找程序
        - 不包含 p: 需提供全路径
        - 包含 v: 以数字接收参数
        - 包含 l: 以列表接收参数
        - 包含 e: 以数字接收环境变量
- 进程树
    - ps -ef: 用户进程不带中括号, 内核进程带中括号
    - 用户进程祖先(1号进程, systemd); 内核进程祖先(2号进程, kthreadd)
    - tty ? 一般表示后台服务


## # 进程管理-线程


#### 为什么要有线程？
- 对于任何一个进程来讲，即便我们没有主动去创建线程，进程也是默认有一个主线程的。线程是负责执行二进制指令的，它会根据项目执行计划书，一行一行执行下去。进程要比线程管的宽多了，除了执行指令之外，内存、文件系统等等都要它来管。

- 进程空间是相互独立的，所以进程间通信代价大

- 进程间共享资源处理比较麻烦


#### 如何创建线程

#### 线程的数据，我们把线程访问的数据细分成三类
- 第一类是线程栈上的本地数据，比如函数执行过程中的局部变量。前面我们说过，函数的调用会使用栈的模型，这在线程里面是一样的。只不过每个线程都有自己的栈空间。栈的大小可以通过命令 ulimit -a 查看，默认情况下线程栈大小为 8192（8MB）。我们可以使用命令 ulimit -s 修改。主线程在内存中有一个栈空间，其他线程栈也拥有独立的栈空间。为了避免线程之间的栈空间踩踏，线程栈之间还会有小块区域，用来隔离保护各自的栈空间。一旦另一个线程踏入到这个隔离区，就会引发段错误。

- 第二类数据就是在整个进程里共享的全局数据。例如全局变量，虽然在不同进程中是隔离的，但是在一个进程中是共享的。如果同一个全局变量，两个线程一起修改，那肯定会有问题，有可能把数据改的面目全非。这就需要有一种机制来保护他们。

- 这就是第三类数据，线程私有数据（Thread Specific Data）


#### 数据的保护 共享的数据保护问题
- 第一种方式，Mutex，全称 Mutual Exclusion，中文叫互斥。顾名思义，有你没我，有我没你。它的模式就是在共享数据访问的时候，去申请加把锁，谁先拿到锁，谁就拿到了访问权限，其他人就只好在门外等着，等这个人访问结束，把锁打开，其他人再去争夺，还是遵循谁先拿到谁访问。
- 在使用 Mutex 的时候，有个问题是如果使用 pthread_mutex_lock()，那就需要一直在那里等着。如果是 pthread_mutex_trylock()，就可以不用等着，去干点儿别的，但是我怎么知道什么时候回来再试一下，是不是轮到我了呢？能不能在轮到我的时候，通知我一下呢？这其实就是条件变量，也就是说如果没事儿，就让大家歇着，有事儿了就去通知，别让人家没事儿就来问问，浪费大家的时间。条件变量和互斥锁是配合使用的。


#### 总结
- 线程复制执行二进制指令
- 多进程缺点: 创建进程占用资源多; 进程间通信需拷贝内存, 不能共享
- 线程相关操作
    - pthread_exit(A), A 是线程退出的返回值
    - pthread_attr_t 线程属性, 用辅助函数初始化并设置值; 用完需要销毁
    - pthread_create 创建线程, 四个参数(线程对象, 属性, 运行函数, 运行参数)
    - pthread_join 获取线程退出返回值, 多线程依赖 libpthread.so
    - 一个线程退出, 会发送信号给 其他所有同进程的线程
- 线程中有三类数据
    - 线程栈本地数据, 栈大小默认 8MB; 线程栈之间有保护间隔, 若误入会引发段错误
    - 进程共享的全局数据
    - 线程级别的全局变量(线程私有数据, pthread_key_create(key, destructer)); key 所有线程都可以访问, 可填入各自的值(同名不同值的全局变量)
- 数据保护
    - Mutex(互斥), 初始化; lock(没抢到则阻塞)/trylock(没抢到则返回错误码); unlock; destroy
    - 条件变量(通知), 收到通知, 还是要抢锁(由 wait 函数执行); 因此条件变量与互斥锁配合使用
    - 互斥锁所谓条件变量的参数, wait 函数会自动解锁/加锁
    - broadcast(通知); destroy


## # 进程管理-进程数据结构（task_struct）（上）

#### 任务 ID
在 Linux 里面，无论是进程，还是线程，到了内核里面，我们统一都叫任务（Task），由一个统一的结构 task_struct 进行管理。这个结构非常复杂。这样就有两个问题:
    - 第一个问题是，任务展示。ps 命令可以展示出所有的进程。但是如果你是这个命令的实现者，到了内核，按照上面的任务列表把这些命令都显示出来，把所有的线程全都平摊开来显示给用户。用户肯定觉得既复杂又困惑。复杂在于，列表这么长；困惑在于，里面出现了很多并不是自己创建的线程。
    - 第二个问题是，给任务下发指令。如：通过 kill 来给进程发信号，通知进程退出。如果发给了其中一个线程，我们就不能只退出这个线程，而是应该退出整个进程。当然，有时候，我们希望只给某个线程发信号。
所以在内核中，它们虽然都是任务，但是应该加以区分。其中，pid 是 process id，tgid 是 thread group ID。任何一个进程，如果只有主线程，那 pid 是自己，tgid 是自己，group_leader 指向的还是自己。但是，如果一个进程创建了其他线程，那就会有所变化了。线程有自己的 pid，tgid 就是进程的主线程的 pid，group_leader 指向的就是进程的主线程。有了 tgid，我们就知道 tast_struct 代表的是一个进程还是代表一个线程了。


#### 信号处理（详细可查看信号相关笔记）
- 任务结构里定义了哪些信号被阻塞暂不处理（blocked），哪些信号尚等待处理（pending），哪些信号正在通过信号处理函数进行处理（sighand）。处理的结果可以是忽略，可以是结束进程等等。
- 信号处理函数默认使用用户态的函数栈，当然也可以开辟新的栈专门用于信号处理，这就是 sas_ss_xxx 这三个变量的作用。
- task_struct 里面有一个 struct sigpending pending。如果我们进入 struct signal_struct *signal 去看的话，还有一个 struct sigpending shared_pending。它们一个是本任务的，一个是线程组共享的。

#### 任务状态
在 task_struct 里面，涉及任务状态的是下面这几个变量：
    - volatile long state;
    - int exit_state;
    - unsigned int flags;
state（状态）可以取的值定义在 include/linux/sched.h 头文件中。
```c

/* Used in tsk->state: */
#define TASK_RUNNING                    0
#define TASK_INTERRUPTIBLE              1
#define TASK_UNINTERRUPTIBLE            2
#define __TASK_STOPPED                  4
#define __TASK_TRACED                   8
/* Used in tsk->exit_state: */
#define EXIT_DEAD                       16
#define EXIT_ZOMBIE                     32
#define EXIT_TRACE                      (EXIT_ZOMBIE | EXIT_DEAD)
/* Used in tsk->state again: */
#define TASK_DEAD                       64
#define TASK_WAKEKILL                   128
#define TASK_WAKING                     256
#define TASK_PARKED                     512
#define TASK_NOLOAD                     1024
#define TASK_NEW                        2048
#define TASK_STATE_MAX                  4096
#define TASK_KILLABLE                   (TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
```
从定义的数值很容易看出来，state 是通过 bitset 的方式设置的，也就是说，当前是什么状态，哪一位就置一。

- TASK_RUNNING
TASK_RUNNING 并不是说进程正在运行，而是表示进程在时刻准备运行的状态。当处于这个状态的进程获得时间片的时候，就是在运行中；如果没有获得时间片，就说明它被其他进程抢占了，在等待再次分配时间片。在运行中的进程，一旦要进行一些 I/O 操作，需要等待 I/O 完毕，这个时候会释放 CPU，进入睡眠状态。

- 在 Linux 中，有两种睡眠状态，具体来说是三种。
    - 一种是 TASK_INTERRUPTIBLE，可中断的睡眠状态。这是一种浅睡眠的状态，也就是说，虽然在睡眠，等待 I/O 完成，但是这个时候一个信号来的时候，进程还是要被唤醒。只不过唤醒后，不是继续刚才的操作，而是进行信号处理。当然程序员可以根据自己的意愿，来写信号处理函数，例如收到某些信号，就放弃等待这个 I/O 操作完成，直接退出；或者收到某些信息，继续等待。
    - 另一种睡眠是 TASK_UNINTERRUPTIBLE，不可中断的睡眠状态。这是一种深度睡眠状态，不可被信号唤醒，只能死等 I/O 操作完成。一旦 I/O 操作因为特殊原因不能完成，这个时候，谁也叫不醒这个进程了。你可能会说，我 kill 它呢？别忘了，kill 本身也是一个信号，既然这个状态不可被信号唤醒，kill 信号也被忽略了。除非重启电脑，没有其他办法。因此，这其实是一个比较危险的事情，除非程序员极其有把握，不然还是不要设置成 TASK_UNINTERRUPTIBLE。
    - 于是，就有了一种新的进程睡眠状态，TASK_KILLABLE，可以终止的新睡眠状态。进程处于这种状态中，它的运行原理类似 TASK_UNINTERRUPTIBLE，只不过可以响应致命信号。TASK_WAKEKILL 用于在接收到致命信号时唤醒进程，而 TASK_KILLABLE 相当于这两位都设置了。

- TASK_STOPPED
是在进程接收到 SIGSTOP、SIGTTIN、SIGTSTP 或者 SIGTTOU 信号之后进入该状态。

- TASK_TRACED 
表示进程被 debugger 等进程监视，进程执行被调试程序所停止。当一个进程被另外的进程所监视，每一个信号都会让进程进入该状态。

- EXIT_ZOMBIE
一旦一个进程要结束，先进入的是 EXIT_ZOMBIE 状态，但是这个时候它的父进程还没有使用 wait() 等系统调用来获知它的终止信息，此时进程就成了僵尸进程。

- EXIT_DEAD 
是进程的最终状态。

- EXIT_ZOMBIE 和 EXIT_DEAD
也可以用于 exit_state。

- 上面的进程状态和进程的运行、调度有关系，还有其他的一些状态，称为标志。放在 flags 字段中，这些字段都被定义成为宏，以 PF 开头。我这里举几个例子。
```c
#define PF_EXITING    0x00000004
#define PF_VCPU      0x00000010
#define PF_FORKNOEXEC    0x00000040
```
    - PF_EXITING 表示正在退出。当有这个 flag 的时候，在函数 find_alive_thread 中，找活着的线程，遇到有这个 flag 的，就直接跳过。
    - PF_VCPU 表示进程运行在虚拟 CPU 上。在函数 account_system_time 中，统计进程的系统运行时间，如果有这个 flag，就调用 account_guest_time，按照客户机的时间进行统计。
    - PF_FORKNOEXEC 表示 fork 完了，还没有 exec。在 _do_fork 函数里面调用 copy_process，这个时候把 flag 设置为 PF_FORKNOEXEC。当 exec 中调用了 load_elf_binary 的时候，又把这个 flag 去掉。


#### 进程调度
进程的状态切换往往涉及调度，具体查看调度模块笔记


#### 总结
- 内核中进程, 线程统一为任务, 由 taks_struct 表示
- 通过链表串起 task_struct
- task_struct 中包含: 任务ID; 任务状态; 信号处理相关字段; 调度相关字段; 亲缘关系; 权限相关; 运行统计; 内存管理; 文件与文件系统; 内核栈;
- 任务 ID; 包含 pid, tgid 和 \*group_leader
    - pid(process id, 线程的id); tgid(thread group id, 所属进程[主线程]的id); group_leader 指向 tgid 的结构体
    - 通过对比 pid 和 tgid 可判断是进程还是线程
- 信号处理, 包含阻塞暂不处理; 等待处理; 正在处理的信号
    - 信号处理函数默认使用用户态的函数栈, 也可以开辟新的栈专门用于信号处理, 由 sas_ss_xxx 指定
    - 通过 pending/shared_pending 区分进程和线程的信号
- 任务状态; 包含 state; exit_state; flags
    - 准备运行状态 TASK_RUNNING
    - 睡眠状态：可中断; 不可中断; 可杀
        - 可中断 TASK_INTERRUPTIBLE, 收到信号要被唤醒
        - 不可中断 TASK_UNINTERRUPTIBLE, 收到信号不会被唤醒, 不能被kill, 只能重启
        - 可杀 TASK_KILLABLE, 可以响应致命信号, 由不可中断与 TASK_WAKEKILL 组合
    - 停止状态 TASK_STOPPED, 由信号 SIGSTOP, SIGTTIN, SIGTSTP 与 SIGTTOU 触发进入
    - 调试跟踪 TASK_TRACED， 被 debugger 等进程监视时进入
    - 结束状态(包含 exit_state)
        - EXIT_ZOMBIE, 父进程还没有 wait()
        - EXIT_DEAD, 最终状态
    - flags, 例如 PF_VCPU 表示运行在虚拟 CPU 上; PF_FORKNOEXEC \_do_fork 函数里设置, exec 函数中清除
- 进程调度; 包含 是否在运行队列; 优先级; 调度策略; 可以使用那些 CPU 等信息.


## # 进程管理-进程数据结构（task_struct）（中）


#### 运行统计信息
在进程的运行过程中，会有一些统计量，具体你可以看下面的列表。这里面有进程在用户态和内核态消耗的时间、上下文切换的次数等等。
```c

u64        utime;//用户态消耗的CPU时间
u64        stime;//内核态消耗的CPU时间
unsigned long      nvcsw;//自愿(voluntary)上下文切换计数
unsigned long      nivcsw;//非自愿(involuntary)上下文切换计数
u64        start_time;//进程启动时间，不包含睡眠时间
u64        real_start_time;//进程启动时间，包含睡眠时间
```

#### 进程亲缘关系
从创建进程的过程，可以看出，任何一个进程都有父进程。所以，整个进程其实就是一棵进程树。而拥有同一父进程的所有进程都具有兄弟关系。
```c
struct task_struct __rcu *real_parent; /* real parent process */
struct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */
struct list_head children;      /* list of my children */
struct list_head sibling;       /* linkage in my parent's children list */
```
    - parent 指向其父进程。当它终止时，必须向它的父进程发送信号。
    - children 表示链表的头部。链表中的所有元素都是它的子进程。
    - sibling 用于把当前进程插入到兄弟链表中。
通常情况下，real_parent 和 parent 是一样的，但是也会有另外的情况存在。例如，bash 创建一个进程，那进程的 parent 和 real_parent 就都是 bash。如果在 bash 上使用 GDB 来 debug 一个进程，这个时候 GDB 是 parent，bash 是这个进程的 real_parent。


#### 进程权限
在 Linux 里面，对于进程权限的定义如下：
```c
/* Objective and real subjective task credentials (COW): */
const struct cred __rcu         *real_cred;
/* Effective (overridable) subjective task credentials (COW): */
const struct cred __rcu         *cred;
```
- 这个结构的注释里，有两个名词比较拗口，Objective 和 Subjective。事实上，所谓的权限，就是我能操纵谁，谁能操纵我。
- “谁能操作我”，很显然，这个时候我就是被操作的对象，就是 Objective，那个想操作我的就是 Subjective。“我能操作谁”，这个时候我就是 Subjective，那个要被我操作的就是 Objectvie。
- “操作”，就是一个对象对另一个对象进行某些动作。当动作要实施的时候，就要审核权限，当两边的权限匹配上了，就可以实施操作。其中，real_cred 就是说明谁能操作我这个进程，而 cred 就是说明我这个进程能够操作谁。


- cred 的定义大部分是关于用户和用户所属的用户组信息。


#### 内存管理
每个进程都有自己独立的虚拟内存空间，这需要有一个数据结构来表示，就是 mm_struct。具体查看内存管理相关笔记。
```c
struct mm_struct                *mm;
struct mm_struct                *active_mm;
```

#### 文件与文件系统
每个进程有一个文件系统的数据结构，还有一个打开文件的数据结构。具体查看文件系统相关笔记。
```c
/* Filesystem information: */
struct fs_struct                *fs;
/* Open file information: */
struct files_struct             *files;
```


#### 总结

- 重点记住以下两点：
    - 进程亲缘关系维护的数据结构，是一种很有参考价值的实现方式，在内核中会多个地方出现类似的结构；
    - 进程权限中 setuid 的原理，这一点比较难理解，但是很重要，面试经常会考。

- 运行统计信息, 包含用户/内核态运行时间; 上/下文切换次数; 启动时间等;
- 进程亲缘关系
    - 拥有同一父进程的所有进程具有兄弟关系
    - 包含: 指向 parent; 指向 real_parent; 子进程双向链表头结点; 兄弟进程双向链表头结点
    - parent 指向的父进程接收进程结束信号
    - real_parent 和 parent 通常一样; 但在 bash 中用 GDB 调试程序时, GDB 是 real_parent, bash 是 parent
- 进程权限, 包含 real_cred 指针(谁能操作我); cred 指针(我能操作谁)
    - cred 结构体中标明多组用户和用户组 id
    - uid/gid(哪个用户的进程启动我)
    - euid/egid(按照哪个用户审核权限, 操作消息队列, 共享内存等)
    - fsuid/fsgid(文件操作时审核)
    - 这三组 id 一般一样
    - 通过 chmod u+s program, 给程序设置 set-user-id 标识位, 运行时程序将进程 euid/fsuid 改为程序文件所有者 id
    - suid/sgid 可以用来保存 id, 进程可以通过 setuid 更改 uid
    - capability 机制, 以细粒度赋予普通用户部分高权限 (capability.h 列出了权限)
        - cap_permitted 表示进程的权限
        - cap_effective 实际起作用的权限, cap_permitted 范围可大于 cap_effective
        - cap_inheritable 若权限可被继承, 在 exec 执行时继承的权限集合, 并加入 cap_permitted 中(但非 root 用户不会保留 cap_inheritable 集合)
        - cap_bset 所有进程保留的权限(限制只用一次的功能)
        - cap_ambient exec 时, 并入 cap_permitted 和 cap_effective 中
- 内存管理: mm_struct
- 文件与文件系统: 打开的文件, 文件系统相关数据结构


## # 进程管理-进程数据结构（task_struct）（下）
在程序执行过程中，一旦调用到系统调用，就需要进入内核继续执行。那如何将用户态的执行和内核态的执行串起来呢？这就需要 task_struct 以下两个重要的成员变量：
```c
struct thread_info thread_info;
void *stack;
```

#### 用户态函数栈
函数调用其实也很简单。如果你去看汇编语言的代码，其实就是指令跳转，从代码的一个地方跳到另外一个地方。数据结构里学的栈，也是后进先出的，所以用栈保存这些最合适。在进程的内存空间里面，栈是一个从高地址到低地址，往下增长的结构，也就是上面是栈底，下面是栈顶，入栈和出栈的操作都是从下面的栈顶开始的。

- 32 位操作系统的情况。在 CPU 里，ESP（Extended Stack Pointer）是栈顶指针寄存器，入栈操作 Push 和出栈操作 Pop 指令，会自动调整 ESP 的值。另外有一个寄存器 EBP（Extended Base Pointer），是栈基地址指针寄存器，指向当前栈帧的最底部。
    - 例子：A 调用 B，A 的栈里面包含 A 函数的局部变量，然后是调用 B 的时候要传给它的参数，然后返回 A 的地址，这个地址也应该入栈，这就形成了 A 的栈帧。接下来就是 B 的栈帧部分了，先保存的是 A 栈帧的栈底位置，也就是 EBP。因为在 B 函数里面获取 A 传进来的参数，就是通过这个指针获取的，接下来保存的是 B 的局部变量等等。当 B 返回的时候，返回值会保存在 EAX 寄存器中，从栈中弹出返回地址，将指令跳转回去，参数也从栈中弹出，然后继续执行 A。

- 对于 64 位操作系统，模式多少有些不一样。因为 64 位操作系统的寄存器数目比较多。rax 用于保存函数调用的返回结果。栈顶指针寄存器变成了 rsp，指向栈顶位置。堆栈的 Pop 和 Push 操作会自动调整 rsp，栈基指针寄存器变成了 rbp，指向当前栈帧的起始位置。改变比较多的是参数传递。rdi、rsi、rdx、rcx、r8、r9 这 6 个寄存器，用于传递存储函数调用时的 6 个参数。如果超过 6 的时候，还是需要放到栈里面。然而，前 6 个参数有时候需要进行寻址，但是如果在寄存器里面，是没有地址的，因而还是会放到栈里面，只不过放到栈里面的操作是被调用函数做的。

- 以上的栈操作，都是在进程的内存空间里面进行的。


#### 内核态函数栈
通过系统调用，从进程的内存空间到内核中。内核中也有各种各样的函数调用来调用去的，也需要这样一个机制，这该怎么办呢？这时候，上面的成员变量 stack，也就是内核栈，就派上了用场。

Linux 给每个 task 都分配了内核栈。
- 在 32 位系统上 arch/x86/include/asm/page_32_types.h，是这样定义的：一个 PAGE_SIZE 是 4K，左移一位就是乘以 2，也就是 8K。
```c
#define THREAD_SIZE_ORDER  1
#define THREAD_SIZE    (PAGE_SIZE << THREAD_SIZE_ORDER)
```
- 内核栈在 64 位系统上 arch/x86/include/asm/page_64_types.h，是这样定义的：在 PAGE_SIZE 的基础上左移两位，也即 16K，并且要求起始地址必须是 8192 的整数倍。
```c
#ifdef CONFIG_KASAN
#define KASAN_STACK_ORDER 1
#else
#define KASAN_STACK_ORDER 0
#endif


#define THREAD_SIZE_ORDER  (2 + KASAN_STACK_ORDER)
#define THREAD_SIZE  (PAGE_SIZE << THREAD_SIZE_ORDER)
```

- 内核栈是一个非常特殊的结构
这段空间的最低位置，是一个 thread_info 结构。这个结构是对 task_struct 结构的补充。因为 task_struct 结构庞大但是通用，不同的体系结构就需要保存不同的东西，所以往往与体系结构有关的，都放在 thread_info 里面。



#### 总结
- 用户态/内核态切换执行如何串起来
- 用户态函数栈; 通过 JMP + 参数 + 返回地址 调用函数
    - 栈内存空间从高到低增长
    - 32位栈结构: 栈帧包含 前一个帧的 EBP + 局部变量 + N个参数 + 返回地址
        - ESP: 栈顶指针; EBP: 栈基址(栈帧最底部, 局部变量起始)
        - 返回值保存在 EAX 中
    - 64位栈结构: 结构类似
        - rax 保存返回结果; rsp 栈顶指针; rbp 栈基指针
        - 参数传递时, 前 6个放寄存器中(再由被调用函数 push 进自己的栈, 用以寻址), 参数超过 6个压入栈中
- 内核栈结构: 
    - Linux 为每个 task 分配了内核栈, 32位(8K), 64位(16K)
    - 栈结构: [预留8字节 +] pt_regs + 内核栈 + 头部 thread_info
    - thread_info 是 task_struct 的补充, 存储于体系结构有关的内容
    - pt_regs 用以保存用户运行上下文, 通过 push 寄存器到栈中保存
    - 通过 task_struct 找到内核栈
        - 直接由 task_struct 内的 stack 直接得到指向 thread_info 的指针
    - 通过内核栈找到 task_struct
        - 32位 直接由 thread_info 中的指针得到
        - 64位 每个 CPU 当前运行进程的 task_struct 的指针存放到 Per CPU 变量 current_task 中; 可调用 this_cpu_read_stable 进行读取


## # 进程管理-调度（上）

#### 调度策略与调度类

1. 一种称为实时进程，也就是需要尽快执行返回结果的那种。这就好比我们是一家公司，接到的客户项目需求就会有很多种。有些客户的项目需求比较急，比如一定要在一两个月内完成的这种，客户会加急加钱，那这种客户的优先级就会比较高。

2. 另一种是普通进程，大部分的进程其实都是这种。这就好比，大部分客户的项目都是普通的需求，可以按照正常流程完成，优先级就没实时进程这么高，但是人家肯定也有确定的交付日期。

3. 策略，在 task_struct 中，有一个成员变量，我们叫调度策略。`unsigned int policy;`，取值枚举有：
```c
#define SCHED_NORMAL    0
#define SCHED_FIFO    1
#define SCHED_RR    2
#define SCHED_BATCH    3
#define SCHED_IDLE    5
#define SCHED_DEADLINE    6
```

4. 配合调度策略的，还有刚才说的优先级，也在 task_struct 中。
```c
int prio, static_prio, normal_prio;
unsigned int rt_priority;
```
优先级其实就是一个数值，对于实时进程，优先级的范围是 0～99；对于普通进程，优先级的范围是 100～139。数值越小，优先级越高。从这里可以看出，所有的实时进程都比普通进程优先级要高。


#### 实时调度策略
对于调度策略，其中 SCHED_FIFO、SCHED_RR、SCHED_DEADLINE 是实时进程的调度策略。

1. SCHED_FIFO 就是交了相同钱的，先来先服务，但是有的加钱多，可以分配更高的优先级，也就是说，高优先级的进程可以抢占低优先级的进程，而相同优先级的进程，我们遵循先来先得。

2. 轮换着来，这就是 SCHED_RR 轮流调度算法，采用时间片，相同优先级的任务当用完时间片会被放到队列尾部，以保证公平性，而高优先级的任务也是可以抢占低优先级的任务。

3. 还有一种新的策略是 SCHED_DEADLINE，是按照任务的 deadline 进行调度的。当产生一个调度点的时候，DL 调度器总是选择其 deadline 距离当前时间点最近的那个任务，并调度它执行。


#### 普通调度策略
对于普通进程的调度策略有，SCHED_NORMAL、SCHED_BATCH、SCHED_IDLE。

1. SCHED_NORMAL 是普通的进程

2. SCHED_BATCH 是后台进程，几乎不需要和前端进行交互。

3. SCHED_IDLE 是特别空闲的时候才跑的进程。


#### 调度策略的执行逻辑
在 task_struct 里面，还有这样的成员变量：`const struct sched_class *sched_class;`，调度策略的执行逻辑，就封装在这里面，它是真正干活的那个。sched_class 有几种实现：
1. stop_sched_class 优先级最高的任务会使用这种策略，会中断所有其他线程，且不会被其他任务打断；
2. dl_sched_class 就对应上面的 deadline 调度策略；
3. rt_sched_class 就对应 RR 算法或者 FIFO 算法的调度策略，具体调度策略由进程的 task_struct->policy 指定；
4. fair_sched_class 就是普通进程的调度策略；
5. idle_sched_class 就是空闲进程的调度策略。
这里实时进程的调度策略 RR 和 FIFO 相对简单一些，而且由于咱们平时常遇到的都是普通进程，在这里，咱们就重点分析普通进程的调度问题。普通进程使用的调度策略是 fair_sched_class，顾名思义，对于普通进程来讲，公平是最重要的。


#### 完全公平调度算法
在 Linux 里面，实现了一个基于 CFS 的调度算法。CFS 全称 Completely Fair Scheduling，叫完全公平调度。



#### 总结
- 调度策略与调度类
	- 进程包括两类: 实时进程(优先级高); 普通进程
	- 两种进程调度策略不同: task_struct->policy 指明采用哪种调度策略(有6种策略)
	- 优先级配合调度策略, 实时进程(0-99); 普通进程(100-139)
	- 实时调度策略, 高优先级可抢占低优先级进程
		- FIFO: 相同优先级进程先来先得
		- RR: 轮流调度策略, 采用时间片轮流调度相同优先级进程
		- Deadline: 在调度时, 选择 deadline 最近的进程
	- 普通调度策略
		- normal: 普通进程
		- batch: 后台进程, 可以降低优先级
		- idle: 空闲时才运行
	- 调度类: task_struct 中 * sched_class 指向封装了调度策略执行逻辑的类(有5种)
		- stop: 优先级最高. 将中断其他所有进程, 且不能被打断
		- dl: 实现 deadline 调度策略
		- rt: RR 或 FIFO, 具体策略由 task_struct->policy 指定
		- fair: 普通进程调度
		- idle: 空闲进程调度
- 普通进程的 fair 完全公平调度算法 CFS(Linux 实现)
	- 记录进程运行时间( vruntime 虚拟运行时间)
	- 优先调度 vruntime 小的进程
	- 按照比例累计 vruntime, 使之考虑进优先级关系
- 调度队列和调度实体
	- CFS 中需要对 vruntime 排序找最小, 不断查询更新, 因此利用红黑树实现调度队列
	- task_struct 中有 实时, deadline 和 cfs 三个调度实体, cfs 调度实体即红黑树节点
	- 每个 CPU 都有 rq 结构体, 里面有 dl_rq, rt_rq 和 cfs_rq 三个调度队列以及其他信息; 队列描述该 CPU 所运行的所有进程
	- 先在 rt_rq 中找进程运行, 若没有再到 cfs_rq 中找; cfs_rq 中 rb_root 指向红黑树根节点, rb_leftmost指向最左节点
- 调度类如何工作
	- 调度类中有一个成员指向下一个调度类(按优先级顺序串起来)
	- 找下一个运行任务时, 按 stop-dl-rt-fair-idle 依次调用调度类, 不同调度类操作不同调度队列




## # 进程管理-调度（中）
所谓进程调度，其实就是一个人在做 A 项目，在某个时刻，换成做 B 项目去了。发生这种情况，主要有两种方式。方式一：A 项目做着做着，发现里面有一条指令 sleep，也就是要休息一下，或者在等待某个 I/O 事件。那没办法了，就要主动让出 CPU，然后可以开始做 B 项目。方式二：A 项目做着做着，旷日持久，实在受不了了。项目经理介入了，说这个项目 A 先停停，B 项目也要做一下，要不然 B 项目该投诉了。

#### 主动调度


#### 总结
- 调度, 切换运行进程, 有两种方式
    - 进程调用 sleep 或等待 I/O, 主动让出 CPU
    - 进程运行一段时间, 被动让出 CPU
- 主动让出 CPU 的方式, 调用 schedule(), schedule() 调用 __schedule()
    - __schedule() 取出 rq; 取出当前运行进程的 task_struct
    - 调用 pick_next_task 取下一个进程
        - 依次调用调度类(优化: 大部分都是普通进程), 因此大多数情况调用 fair_sched_class.pick_next_task[_fair]
        - pick_next_task_fair 先取出 cfs_rq 队列, 取出当前运行进程调度实体, 更新 vruntime
        - pick_next_entity 取最左节点, 并得到 task_struct, 若与当前进程不一样, 则更新红黑树 cfs_rq
    - 进程上下文切换: 切换进程内存空间, 切换寄存器和 CPU 上下文(运行 context_switch)
        - context_switch() -> switch_to() -> __switch_to_asm(切换[内核]栈顶指针) -> __switch_to()
        - __switch_to() 取出 Per CPU 的 tss(任务状态段) 结构体
        - > x86 提供以硬件方式切换进程的模式, 为每个进程在内存中维护一个 tss, tss 有所有寄存器, 同时 TR(任务寄存器)指向某个 tss, 更改 TR 会触发换出 tss(旧进程)和换入 tss(新进程), 但切换进程没必要换所有寄存器
        - 因此 Linux 中每个 CPU 关联一个 tss, 同时 TR 不变, Linux 中参与进程切换主要是栈顶寄存器
        - task_struct 的 thread 结构体保留切换时需要修改的寄存器, 切换时将新进程 thread 写入 CPU tss 中
        - 具体各类指针保存位置和时刻
            - 用户栈, 切换进程内存空间时切换
            - 用户栈顶指针, 内核栈 pt_regs 中弹出
            - 用户指令指针, 从内核栈 pt_regs 中弹出
            - 内核栈, 由切换的 task_struct 中的 stack 指针指向
            - 内核栈顶指针, __switch_to_asm 函数切换(保存在 thread 中)
            - 内核指令指针寄存器: 进程调度最终都会调用 __schedule, 因此在让出(从当前进程)和取得(从其他进程) CPU 时, 该指针都指向同一个代码位置.



## # 进程管理-调度（下）
主动调度，就是进程运行到一半，因为等待 I/O 等操作而主动让出 CPU，然后就进入了“进程调度第一定律”。所有进程的调用最终都会走 __schedule 函数。这个定律在抢占式调度还是要继续起作用。


#### 抢占式调度-什么情况下会发生抢占呢？

1. 最常见的现象就是一个进程执行时间太长了，是时候切换到另一个进程了。

那怎么衡量一个进程的运行时间呢？在计算机里面有一个时钟，会过一段时间触发一次时钟中断，通知操作系统，时间又过去一个时钟周期，这是个很好的方式，可以查看是否是需要抢占的时间点。

时钟中断处理函数会调用 scheduler_tick()，这个函数先取出当前 CPU 的运行队列，然后得到这个队列上当前正在运行中的进程的 task_struct，然后调用这个 task_struct 的调度类的 task_tick 函数，顾名思义这个函数就是来处理时钟事件的。如果当前运行的进程是普通进程，调度类为 fair_sched_class，调用的处理时钟的函数为 task_tick_fair。根据当前进程的 task_struct，找到对应的调度实体 sched_entity 和 cfs_rq 队列，调用 entity_tick。

在 entity_tick 里面，又见到了熟悉的 update_curr。它会更新当前进程的 vruntime，然后调用 check_preempt_tick。顾名思义就是，检查是否是时候被抢占了。除了这个条件之外，还会通过 __pick_first_entity 取出红黑树中最小的进程。如果当前进程的 vruntime 大于红黑树中最小的进程的 vruntime，且差值大于 ideal_runtime，也应该被抢占了。

当发现当前进程应该被抢占，不能直接把它踢下来，而是把它标记为应该被抢占。为什么呢？因为进程调度第一定律呀，一定要等待正在运行的进程调用 __schedule 才行啊，所以这里只能先标记一下。标记一个进程应该被抢占，都是调用 resched_curr，它会调用 set_tsk_need_resched，标记进程应该被抢占，但是此时此刻，并不真的抢占，而是打上一个标签 TIF_NEED_RESCHED。

2. 另外一个可能抢占的场景是当一个进程被唤醒的时候。
当一个进程在等待一个 I/O 的时候，会主动放弃 CPU。但是当 I/O 到来的时候，进程往往会被唤醒。这个时候是一个时机。当被唤醒的进程优先级高于 CPU 上的当前进程，就会触发抢占。try_to_wake_up() 调用 ttwu_queue 将这个唤醒的任务添加到队列当中。ttwu_queue 再调用 ttwu_do_activate 激活这个任务。ttwu_do_activate 调用 ttwu_do_wakeup。这里面调用了 check_preempt_curr 检查是否应该发生抢占。如果应该发生抢占，也不是直接踢走当前进程，而是将当前进程标记为应该被抢占。

#### 抢占式调度-抢占的时机
真正的抢占还需要时机，也就是需要那么一个时刻，让正在运行中的进程有机会调用一下 __schedule。可以想象，不可能某个进程代码运行着，突然要去调用 __schedule，代码里面不可能这么写，所以一定要规划几个时机，这个时机分为用户态和内核态。

1. 用户态的抢占时机
    - 对于用户态的进程来讲，从系统调用中返回的那个时刻，是一个被抢占的时机。
    - 对于用户态的进程来讲，从中断中返回的那个时刻，也是一个被抢占的时机。

2. 内核态的抢占时机
    - 对内核态的执行中，被抢占的时机一般发生在 preempt_enable() 中。在内核态的执行中，有的操作是不能被中断的，所以在进行这些操作之前，总是先调用 preempt_disable() 关闭抢占，当再次打开的时候，就是一次内核态代码被抢占的机会。
    - 在内核态也会遇到中断的情况，当中断返回的时候，返回的仍然是内核态。这个时候也是一个执行抢占的时机。


#### 总结
- 抢占式调度
- 两种情况: 执行太久, 需切换到另一进程; 另一个高优先级进程被唤醒
    - 执行太久: 由时钟中断触发检测, 中断处理调用 scheduler_tick 
        - 取当前进程  task_struct->task_tick_fair()->取 sched_entity cfs_rq 调用 entity_tick()
        - entity_tick() 调用 update_curr 更新当前进程 vruntime, 调用 check_preempt_tick 检测是否需要被抢占
        - check_preempt_tick 中计算 ideal_runtime(一个调度周期中应该运行的实际时间), 若进程本次调度运行时间 > ideal_runtime, 则应该被抢占
        - 要被抢占, 则调用 resched_curr, 设置 TIF_NEED_RESCHED, 将其标记为应被抢占进程(因为要等待当前进程运行 `__schedule`)
    - 另一个高优先级进程被唤醒: 当 I/O 完成, 进程被唤醒, 若优先级高于当前进程则触发抢占
        - try_to_wake_up()->ttwu_queue() 将唤醒任务加入队列 调用 ttwu_do_activate 激活任务
        - 调用 tt_do_wakeup()->check_preempt_curr() 检查是否应该抢占, 若需抢占则标记
- 抢占时机: 让进程调用 `__schedule`, 分为用户态和内核态
    - 用户态进程
        - 时机-1: 从系统调用中返回, 返回过程中会调用 exit_to_usermode_loop, 检查 `_TIF_NEED_RESCHED`, 若打了标记, 则调用 schedule()
        - 时机-2: 从中断中返回, 中断返回分为返回用户态和内核态(汇编代码: arch/x86/entry/entry_64.S), 返回用户态过程中会调用 exit_to_usermode_loop()->shcedule()
    - 内核态进程
        - 时机-1: 发生在 preempt_enable() 中, 内核态进程有的操作不能被中断, 会调用 preempt_disable(), 在开启时(调用 preempt_enable) 时是一个抢占时机, 会调用 preempt_count_dec_and_test(), 检测 preempt_count 和标记, 若可抢占则最终调用 `__schedule`
        - 时机-2: 发生在中断返回, 也会调用 `__schedule`


## # 进程管理-进程的创建


#### 总结
- fork -> sys_call_table 转换为 sys_fork()->`_do_fork`
- 创建进程做两件事: 复制初始化 task_struct; 唤醒新进程
- 复制并初始化 task_struct, copy_process()
    - dup_task_struct: 分配 task_struct 结构体; 创建内核栈, 赋给`* stack`; 复制 task_struct, 设置 thread_info;
    - copy_creds: 分配 cred 结构体并复制, p->cred = p->real_cred = get_cred(new)
    - 初始化运行时统计量
    - sched_fork 调度相关结构体: 分配并初始化 sched_entity; state = TASK_NEW; 设置优先级和调度类; task_fork_fair()->update_curr 更新当前进程运行统计量, 将当前进程 vruntime 赋给子进程, 通过 sysctl_sched_child_runs_first 设置是否让子进程抢占, 若是则将其 sched_entity 放前头, 并调用 resched_curr 做被抢占标记.
    - 初始化文件和文件系统变量 
        - copy_files: 复制进程打开的文件信息, 用 files_struct 维护; 
        - copy_fs: 复制进程目录信息, 包括根目录/根文件系统; pwd 等, 用 fs_struct 维护
    - 初始化信号相关内容: 复制信号和处理函数
    - 复制内存空间: 分配并复制 mm_struct; 复制内存映射信息
    - 分配 pid
- 唤醒新进程 wake_up_new_task()
    - state = TASK_RUNNING; activate 用调度类将当前子进程入队列
    - 其中 enqueue_entiry 中会调用 update_curr 更新运行统计量, 再加入队列
    - 调用 check_preempt_curr 看是否能抢占, 若 task_fork_fair 中已设置 sysctl_sched_child_runs_first, 直接返回, 否则进一步比较并调用 resched_curr 做抢占标记
    - 若父进程被标记会被抢占, 则系统调用 fork 返回过程会调度子进程


## # 进程管理-线程的创建

- pstree -apl pid看进程树
- pstack pid 看栈

#### 用户态创建线程

#### 总结
- 线程的创建
- 线程是由内核态和用户态合作完成的, pthread_create 是 Glibc 库的一个函数
- pthread_create 中
1. 设置线程属性参数, 如线程栈大小
2. 创建用户态维护线程的结构, pthread
3. 创建线程栈 allocate_stack
    - 取栈的大小, 在栈末尾加 guardsize
    - 在进程堆中创建线程栈(先尝试调用 get_cached_stack 从缓存回收的线程栈中取用)
    - 若无缓存线程栈, 调用 `__mmap` 创建
    - 将 pthread 指向栈空间中
    - 计算 guard 内存位置, 并设置保护
    - 填充 pthread 内容, 其中 specific 存放属于线程的全局变量
    - 线程栈放入 stack_used 链表中(另外 stack_cache 链表记录回收缓存的线程栈)
4. 设置运行函数, 参数到 pthread 中
5. 调用 create_thread 创建线程
    - 设置 clone_flags 标志位, 调用 `__clone`
    - clone 系统调用返回时, 应该要返回到新线程上下文中, 因此 `__clone` 将参数和指令位置压入栈中, 返回时从该函数开始执行
6. 内核调用 `__do_fork` 
    - 在 copy_process 复制 task_struct 过程中, 五大数据结构不复制, 直接引用进程的
    - 亲缘关系设置: group_leader 和 tgid 是当前进程; real_parent 与当前进程一样
    - 信号处理: 数据结构共享, 处理一样
7. 返回用户态, 先运行 start_thread 同样函数
    - 在 start_thread 中调用用户的函数, 运行完释放相关数据
    - 如果是最后一个线程直接退出
    - 或调用 `__free_tcb` 释放 pthread 以及线程栈, 从 stack_used 移到 stack_cache 中



## # 内存管理-内存管理（上）

#### 规划虚拟地址空间
- 操作系统的内存管理，主要分为三个方面。
    1. 第一，物理内存的管理，相当于会议室管理员管理会议室。
    2. 第二，虚拟地址的管理，也即在项目组的视角，会议室的虚拟地址应该如何组织。
    3. 第三，虚拟地址和物理地址如何映射，也即会议室管理员如何管理映射表。

- 以下是个简单的c程序：

```c
#include <stdio.h>
#include <stdlib.h>

int max_length = 128;

char * generate(int length){
  int i;
  char * buffer = (char*) malloc (length+1);
  if (buffer == NULL)
    return NULL;
  for (i=0; i<length; i++){
    buffer[i]=rand()%26+'a';
  }
  buffer[length]='\0';
  return buffer;
}

int main(int argc, char *argv[])
{
  int num;
  char * buffer;

  printf ("Input the string length : ");
  scanf ("%d", &num);

  if(num > max_length){
    num = max_length;
  }

  buffer = generate(num);

  printf ("Random string is: %s\n",buffer);
  free (buffer);

  return 0;
}

```
- 这个简单的程序在使用内存时的几种方式
    1. 代码需要放在内存里面；
    2. 全局变量；
    3. 常量字符串；
    4. 函数栈，例如局部变量 num 是作为参数传给 generate 函数的，这里面涉及了函数调用，局部变量，函数参数等都是保存在函数栈上面的；
    5. 堆，malloc 分配的内存在堆里面；
    6. 这里面涉及对 glibc 的调用，所以 glibc 的代码是以 so 文件的形式存在的，也需要放在内存里面。

    7. 内核执行代码时也要分配：
        1. 内核的代码要在内存里面；
        2. 内核中也有全局变量；
        3. 每个进程都要有一个 task_struct；
        4. 每个进程还有一个内核栈；
        5. 在内核里面也有动态分配的内存；
    8. 虚拟地址到物理地址的映射表


#### 现在站在一个进程的角度去看这个虚拟的空间，不用管其他进程。  -- 用户空间
- 首先，这么大的虚拟空间一切二，一部分用来放内核的东西，称为内核空间，一部分用来放进程的东西，称为用户空间。用户空间在下，在低地址，我们假设就是 0 号到 29 号会议室；内核空间在上，在高地址，我们假设是 30 号到 39 号会议室。这两部分空间的分界线因为 32 位和 64 位的不同而不同，这里不深究。

- 对于普通进程来说，内核空间的那部分虽然虚拟地址在那里，但是不能访问。这就像作为普通员工，你明明知道财务办公室在这个 30 号会议室门里面，但是门上挂着“闲人免进”，你只能在自己的用户空间里面折腾。

- 从最低位开始排起，先是 Text Segment、Data Segment 和 BSS Segment。Text Segment 是存放二进制可执行代码的位置，Data Segment 存放静态常量，BSS Segment 存放未初始化的静态变量。是不是觉得这几个名字很熟悉？没错，之前 ELF 格式文件的笔记提到过，在二进制执行文件里面，就有这三个部分。这里就是把二进制执行文件的三个部分加载到内存里面。

- 接下来是堆（Heap）段。堆是往高地址增长的，是用来动态分配内存的区域，malloc 就是在这里面分配的。

- 再下面就是栈（Stack）地址段。主线程的函数调用的函数栈就是用这里的。

如果普通进程还想进一步访问内核空间，是没办法的，只能眼巴巴地看着。如果需要进行更高权限的工作，就需要调用系统调用，进入内核。


#### 现在站在一个进程的角度去看这个虚拟的空间。  -- 内核空间
- 一旦进入了内核，就换了一种视角。刚才是普通进程的视角，觉着整个空间是它独占的，没有其他进程存在。当然另一个进程也这样认为，因为它们互相看不到对方。这也就是说，不同进程的 0 号到 29 号会议室放的东西都不一样。

- 但是到了内核里面，无论是从哪个进程进来的，看到的都是同一个内核空间，看到的都是同一个进程列表。虽然内核栈是各用各的，但是如果想知道的话，还是能够知道每个进程的内核栈在哪里的。所以，如果要访问一些公共的数据结构，需要进行锁保护。

- 内核的代码访问内核的数据结构，大部分的情况下都是使用虚拟地址的，虽然内核代码权限很大，但是能够使用的虚拟地址范围也只能在内核空间，也即内核代码访问内核数据结构。

- 在内核里面也会有内核的代码，同样有 Text Segment、Data Segment 和 BSS Segment，别忘了咱们讲内核启动的时候，内核代码也是 ELF 格式的。内核的其他数据结构的分配方式就比较复杂了。


#### 总结
- 内存管理包含: 物理内存管理; 虚拟内存管理; 两者的映射
- 除了内存管理模块, 其他都使用虚拟地址(包括内核)
- 虚拟内存空间包含: 内核空间(高地址); 用户空间(低地址)
- 用户空间从低到高布局为: 代码段; DATA 段; BSS 段(未初始化静态变量); 堆段; 内存映射段; 栈地址空间段
- 多个进程看到的用户空间是独立的
- 内核空间: 多个进程看到同一内核空间, 但内核栈每个进程不一样
- 内核代码也仅能访问内核空间
- 内核也有内核代码段, DATA 段, 和 BSS 段; 位于内核空间低地址
- 内核代码也是 ELF 格式



## # 内存管理-内存管理（下）


#### 分段机制
分段机制下的虚拟地址由两部分组成，段选择子和段内偏移量。段选择子就保存在咱们前面讲过的段寄存器里面。段选择子里面最重要的是段号，用作段表的索引。段表里面保存的是这个段的基地址、段的界限和特权等级等。虚拟地址中的段内偏移量应该位于 0 和段界限之间。如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

- 在 Linux 里面，段表全称段描述符表（segment descriptors），放在全局描述符表 GDT（Global Descriptor Table）里面，一个段表项由段基地址 base、段界限 limit，还有一些标识符组成。
- 64 位的和 32 位的，都定义了内核代码段、内核数据段、用户代码段和用户数据段。
- 另外，还会定义四个段选择子，指向上面的段描述符表项。内核初始化的时候，启动第一个用户态的进程，就是将这四个值赋值给段寄存器。

通过分析发现，所有的段的起始地址都是一样的，都是 0。这算哪门子分段嘛！所以，在 Linux 操作系统中，并没有使用到全部的分段功能。那分段是不是完全没有用处呢？分段可以做权限审核，例如用户态 DPL 是 3，内核态 DPL 是 0。当用户态试图访问内核态的时候，会因为权限不足而报错。

#### 分页机制
其实 Linux 倾向于另外一种从虚拟地址到物理地址的转换方式，称为分页（Paging）。

- 对于物理内存，操作系统把它分成一块一块大小相同的页，这样更方便管理，例如有的内存页面长时间不用了，可以暂时写到硬盘上，称为换出。一旦需要的时候，再加载进来，叫做换入。这样可以扩大可用物理内存的大小，提高物理内存的利用率。

- 这个换入和换出都是以页为单位的。页面的大小一般为 4KB。为了能够定位和访问每个页，需要有个页表，保存每个页的起始地址，再加上在页内的偏移量，组成线性地址，就能对于内存中的每个位置进行访问了。

- **虚拟地址分为两部分，页号和页内偏移。页号作为页表的索引，页表包含物理页每页所在物理内存的基地址。这个基地址与页内偏移的组合就形成了物理内存地址。**

- 32 位环境下，虚拟地址空间共 4GB。如果分成 4KB 一个页，那就是 1M 个页。每个页表项需要 4 个字节来存储，那么整个 4GB 空间的映射就需要 4MB 的内存来存储映射表。如果每个进程都有自己的映射表，100 个进程就需要 400MB 的内存。对于内核来讲，有点大了 。

- 页表中所有页表项必须提前建好，并且要求是连续的。如果不连续，就没有办法通过虚拟地址里面的页号找到对应的页表项了。

- 那怎么办呢？可以试着将页表再分页，4G 的空间需要 4M 的页表来存储映射。我们把这 4M 分成 1K（1024）个 4K，每个 4K 又能放在一页里面，这样 1K 个 4K 就是 1K 个页，这 1K 个页也需要一个表进行管理，称为**页目录表**，这个页目录表里面有 1K 项，每项 4 个字节，页目录表大小也是 4K。

- 页目录有 1K 项，用 10 位就可以表示访问页目录的哪一项。这一项其实对应的是一整页的页表项，也即 4K 的页表项。每个页表项也是 4 个字节，因而一整页的页表项是 1K 个。再用 10 位就可以表示访问页表项的哪一项，页表项中的一项对应的就是一个页，是存放数据的页，这个页的大小是 4K，用 12 位可以定位这个页内的任何一个位置。

- **这样加起来正好 32 位，也就是用前 10 位定位到页目录表中的一项。将这一项对应的页表取出来共 1k 项，再用中间 10 位定位到页表中的一项，将这一项对应的存放数据的页取出来，再用最后 12 位定位到页中的具体位置访问数据。**

- 如果这样的话，映射 4GB 地址空间就需要 4MB+4KB 的内存，这样不是更大了吗？ 当然如果页是满的，当时是更大了，但是，我们往往不会为一个进程分配那么多内存。

- 我们假设只给这个进程分配了一个数据页。如果只使用页表，也需要完整的 1M 个页表项共 4M 的内存，但是如果使用了页目录，页目录需要 1K 个全部分配，占用内存 4K，但是里面只有一项使用了。到了页表项，只需要分配能够管理那个数据页的页表项页就可以了，也就是说，最多 4K，这样内存就节省多了。

- 当然对于 64 位的系统，两级肯定不够了，就变成了四级目录，分别是全局页目录项 PGD（Page Global Directory）、上层页目录项 PUD（Page Upper Directory）、中间页目录项 PMD（Page Middle Directory）和页表项 PTE（Page Table Entry）。


#### 总结
- 虚拟内存地址到物理内存地址的映射
- 分段
    - 虚拟地址 = 段选择子(段寄存器) + 段内偏移量
    - 段选择子 = 段号(段表索引) + 标识位
    - 段表 = 物理基地址 + 段界限(偏移量范围) + 特权等级
- Linux 分段实现
    - 段表称为段描述符表, 放在全局标识符表中
    - Linux 将段基地址都初始化为 0, 不用于地址映射
    - Linux 分段功能主要用于权限检查
- Linux 通过分页实现映射
    - 物理内存被换分为大小固定(4KB)的页, 物理页可在内存与硬盘间换出/换入
    - 页表 = 虚拟页号 + 物理页号; 用于定位页
    - 虚拟地址 = 虚拟页号 + 页内偏移
    - 若采用单页表, 32位系统中一个页表将有 1M 页表项, 占用 4MB(每项 4B)
    - Linux 32位系统采用两级页表: 页表目录(1K项, 10bit) + 页表(1K项, 10bit)(页大小(4KB, 12bit))
    - 映射 4GB 内存理论需要 1K 个页表目录项 + 1K*1K=1M 页表项, 将占用 4KB+4MB 空间
    - 因为完整的页表目录可以满足所有地址的查询, 因此页表只需在对应地址有内存分配时才生成;
    - 64 为系统采用 4 级页表


## # 内存管理-进程空间管理


#### 用户态和内核态的划分
进程的虚拟地址空间，就从 task_struct 出发来看。这里面有一个 struct mm_struct 结构来管理内存。在里面，有这样一个成员变量：`unsigned long task_size;`，之前说过，整个虚拟内存空间要一分为二，一部分是用户态地址空间，一部分是内核态地址空间，那这两部分的分界线在哪里呢？这就要 task_size 来定义。

- 对于 32 位系统，最大能够寻址 2^32=4G，其中用户态虚拟地址空间是 3G，内核态是 1G。

- 对于 64 位系统，虚拟地址只使用了 48 位。就像代码里面写的一样，1 左移了 47 位，就相当于 48 位地址空间一半的位置，0x0000800000000000，然后减去一个页，就是 0x00007FFFFFFFF000，共 128T。同样，内核空间也是 128T。内核空间和用户空间之间隔着很大的空隙，以此来进行隔离。


#### 总结

1. 用户态：
    - 代码段、全局变量、BSS
    - 函数栈
    - 堆
    - 内存映射区

2. 内核态
    - 内核的代码、全局变量、BSS
    - 内核数据结构例如 task_struct
    - 内核栈
    - 内核中动态分配的内存

- 内存管理信息在 task_struct 的 mm_struct 中
- task_size 指定用户态虚拟地址大小
    - 32 位系统：3G 用户态, 1G 内核态
    - 64 位系统(只利用 48 bit 地址): 128T 用户态; 128T 内核态
- 用户态地址空间布局和管理
    - mm_struct 中有映射页的统计信息(总页数, 锁定页数, 数据/代码/栈映射页数等)以及各区域地址
    - 有 vm_area_struct 描述各个区域(代码/数据/栈等)的属性(包含起始/终止地址, 可做的操作等), 通过链表和红黑树管理
    - 在 load_elf_bianry 时做 vm_area_struct 与各区域的映射, 并将 elf 映射到内存, 将依赖 so 添加到内存映射
    - 在函数调用时会修改栈顶指针; malloc 分配内存时会修改对应的区域信息(调用 brk 堆; 或调用 mmap 内存映射)
    - brk 判断是否需要分配新页, 并做对应操作; 需要分配新页时需要判断能否与其他 vm_area_struct 合并
    - 32 位和 64 位的空间相差很大，但是区域的类别和布局是相似的。
- 内核地址空间布局和管理
    - 所有进程看到的内核虚拟地址空间是同一个
    - 32 位系统, 前 896MB 为直接映射区(虚拟地址 - 3G = 物理地址)
        - 直接映射区也需要建立页表, 通过虚拟地址访问(除了内存管理模块)
        - 直接映射区组成: 1MB 启动时占用; 然后是内核代码/全局变量/BSS等,即 内核 ELF文件内容; 进程 task_struct 即内核栈也在其中
        - 896MB 也称为高端内存(指物理内存)
        - 剩余虚拟空间组成: 8MB 空余; 内核动态映射空间(动态分配内存, 映射放在内核页表中); 持久内存映射(储存物理页信息); 固定内存映射; 临时内存映射(例如为进程映射文件时使用)
    - 64 位系统: 8T 空余; 64T 直接映射区域; 32T(动态映射); 1T(物理页描述结构 struct page); 512MB(内核代码, 也采用直接映射)


## # 内存管理-物理内存管理（上）


## # 内存管理-物理内存管理（下）


## # 内存管理-用户态内存映射


## # 内存管理-内核态内存映射

