
在 **Ollama** 中对本地大模型进行参数微调需要结合其他工具（如 **Hugging Face Transformers** 或 **llama.cpp**），因为 Ollama 本身主要面向模型部署而非训练。以下是完整的操作指南：

---

### 一、Ollama 微调的限制与解决方案
| **限制**               | **解决方案**                                                                 |
|-------------------------|-----------------------------------------------------------------------------|
| 不支持直接训练          | 使用 Transformers + PEFT 微调后，将模型转换为 GGUF 格式供 Ollama 加载       |
| 显存要求高              | 采用 **4-bit 量化** + **LoRA** 微调技术                                      |
| 模型格式限制            | 通过 `llama.cpp` 将 PyTorch 模型转换为 GGUF 格式                             |

---

### 二、分步操作指南
#### **步骤 1：环境准备**
```powershell
# 安装必要工具链
conda create -n ollama-finetune python=3.10
conda activate ollama-finetune
pip install torch transformers datasets peft bitsandbytes accelerate
```

#### **步骤 2：数据准备**
1. **数据集格式示例**（保存为 `train.jsonl`）：
   ```json
   {"instruction": "用Python实现快速排序", "input": "", "output": "def quicksort(arr):..."}
   ```

2. **数据集预处理脚本**：
   ```python
   # preprocess.py
   from datasets import load_dataset

   dataset = load_dataset("json", data_files="train.jsonl")
   dataset = dataset.map(lambda x: {"text": f"Instruction: {x['instruction']}\nInput: {x['input']}\nOutput: {x['output']}"})
   dataset.save_to_disk("processed_data")
   ```

#### **步骤 3：LoRA 微调**
```python
# finetune_lora.py
from peft import LoraConfig
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    TrainingArguments,
    Trainer
)

# 加载模型（以 DeepSeek-8B 为例）
model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/deepseek-llm-7b-base",
    load_in_4bit=True,  # 4-bit 量化
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-llm-7b-base")

# LoRA 配置
peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# 训练参数
training_args = TrainingArguments(
    output_dir="./output",
    per_device_train_batch_size=2,    # 根据显存调整
    gradient_accumulation_steps=4,     # 模拟更大 batch size
    learning_rate=1e-4,
    num_train_epochs=1,
    fp16=True,
    logging_steps=10,
    save_strategy="steps",
    save_steps=100
)

# 启动训练
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    peft_config=peft_config
)
trainer.train()
trainer.save_model("lora_adapter")  # 保存 LoRA 适配器
```

#### **步骤 4：模型合并与格式转换**
1. **合并 LoRA 权重**：
   ```python
   from peft import PeftModel

   base_model = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-llm-7b-base")
   merged_model = PeftModel.from_pretrained(base_model, "lora_adapter")
   merged_model = merged_model.merge_and_unload()
   merged_model.save_pretrained("merged_model")
   ```

2. **转换为 GGUF 格式**（供 Ollama 使用）：
   ```bash
   git clone https://github.com/ggerganov/llama.cpp
   cd llama.cpp && make

   # 转换 PyTorch 模型 -> GGUF
   python3 convert.py merged_model/ --vocab-type bpe --outfile merged.gguf
   ```

#### **步骤 5：Ollama 加载微调后模型**
1. 创建 `Modelfile`：
   ```dockerfile
   FROM ./merged.gguf
   PARAMETER num_ctx 4096
   PARAMETER temperature 0.7
   ```

2. 创建并运行模型：
   ```bash
   ollama create my-finetuned-model -f Modelfile
   ollama run my-finetuned-model
   ```

---

### 三、关键参数调优建议
| **参数**               | **推荐值**      | **调整策略**                              |
|------------------------|----------------|------------------------------------------|
| `per_device_train_batch_size` | 1-4         | 显存不足时降低值并增加 `gradient_accumulation_steps` |
| `learning_rate`        | 1e-5 到 3e-4   | 小数据集用高学习率，大数据集用低学习率               |
| `lora_rank (r)`        | 8-64          | 任务复杂度越高，rank 值越大                       |
| `max_seq_length`       | 2048-4096     | 根据 GPU 显存调整                              |

---

### 四、显存优化技巧
1. **梯度检查点**（激活重计算）：
   ```python
   model.gradient_checkpointing_enable()  # 减少30%显存，增加20%训练时间
   ```

2. **8-bit 优化器**：
   ```python
   from bitsandbytes.optim import Adam8bit
   training_args.optim = "adamw_8bit"
   ```

3. **分页优化器**（防OOM）：
   ```python
   training_args.optim_args = {"use_paged_optimizer": True}
   ```

---

### 五、验证微调效果
1. **交互式测试**：
   ```bash
   ollama run my-finetuned-model
   > 输入: 用Python写一个快速排序
   ```

2. **批量测试脚本**：
   ```python
   from transformers import pipeline

   pipe = pipeline("text-generation", model="merged_model")
   test_prompts = ["解释量子纠缠", "写一首关于秋天的诗"]
   for prompt in test_prompts:
       print(pipe(f"Instruction: {prompt}\nOutput:"))
   ```

---

### 六、常见问题解决
| **问题**                  | **解决方案**                                                                 |
|---------------------------|-----------------------------------------------------------------------------|
| `CUDA out of memory`       | 降低 `batch_size`，启用 `4-bit` 量化，使用 `gradient_checkpointing`         |
| 生成结果不相关            | 检查数据质量，增加 `num_train_epochs`（建议 1-3 轮）                        |
| Ollama 加载失败           | 确认 GGUF 文件完整性：`md5sum merged.gguf`                                  |
| 微调后模型退化             | 降低学习率，添加更多多样化数据，减少训练步数                                |

---

通过以上方法，可以在 **24GB 显存** 的 GPU 上完成 8B 模型的微调。建议首次运行时先用 100 条数据验证流程，再扩展到全量数据集。